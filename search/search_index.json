{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"analytics-engineering/","text":"What is analytics engineering? In one sentence, I think analytics engineering is: Helping organizations extract value from their data, through collecting, organizing, and interpreting. An analogy is to be the manager of a communal workshop: create a destination where people can be a productive as possible, have the tools to do their best work, and contribute back to the workshop themselves. From one of the founders on stackoverflow: https://news.ycombinator.com/item?id=24656588 \"Our product, dbt (https://www.getdbt.com/), has come to be synonymous with the practice of analytics engineering, defining an entire industry.\" Interpreting Interpreting requires more domain knowledge than anything else. The analytics engineer might know that there was actually an automated QA process that was creating new records in the database, which were essentially bots. Those bots shouldn't be counted in the user growth. They're not just monitoring the data itself, but they're close to the teams working on those projects, so they have the contextual awareness to help with that interpretation. Other types of interpreting include not just \"local knowledge\". Example from FB: tracking sentiment of people that were exposed to FB \"brand campaigns.\" But that's not unique to an analytics engineer. So if interpreting is one of the skills associated with analytics engineering, why is it called engineering? Engineering implies some level of building things - what are they building? Organizing They primary way to ensure people avoid the issue with the data mentioned is to have it organized in a way that doesn't require constantly remembering and add custom resolutions to the data. Imagine if everyone at the organization had to implement their own solution to account for those automated users. It's inevitable someone would forget, or doing it in a way that was subtly wrong, or just make a typo. Most organizations today organize their data in a system they call a data warehouse. Collecting But there's now another problem - what if there's no reliable way to differentiate those bot users? The simplest solution might be to ask the engineers that implemented the automated QA to What analytics engineering is not Building data pipelines Pulling data Predictive modelling","title":"What is analytics engineering?"},{"location":"analytics-engineering/#what-is-analytics-engineering","text":"In one sentence, I think analytics engineering is: Helping organizations extract value from their data, through collecting, organizing, and interpreting. An analogy is to be the manager of a communal workshop: create a destination where people can be a productive as possible, have the tools to do their best work, and contribute back to the workshop themselves. From one of the founders on stackoverflow: https://news.ycombinator.com/item?id=24656588 \"Our product, dbt (https://www.getdbt.com/), has come to be synonymous with the practice of analytics engineering, defining an entire industry.\"","title":"What is analytics engineering?"},{"location":"analytics-engineering/#interpreting","text":"Interpreting requires more domain knowledge than anything else. The analytics engineer might know that there was actually an automated QA process that was creating new records in the database, which were essentially bots. Those bots shouldn't be counted in the user growth. They're not just monitoring the data itself, but they're close to the teams working on those projects, so they have the contextual awareness to help with that interpretation. Other types of interpreting include not just \"local knowledge\". Example from FB: tracking sentiment of people that were exposed to FB \"brand campaigns.\" But that's not unique to an analytics engineer. So if interpreting is one of the skills associated with analytics engineering, why is it called engineering? Engineering implies some level of building things - what are they building?","title":"Interpreting"},{"location":"analytics-engineering/#organizing","text":"They primary way to ensure people avoid the issue with the data mentioned is to have it organized in a way that doesn't require constantly remembering and add custom resolutions to the data. Imagine if everyone at the organization had to implement their own solution to account for those automated users. It's inevitable someone would forget, or doing it in a way that was subtly wrong, or just make a typo. Most organizations today organize their data in a system they call a data warehouse.","title":"Organizing"},{"location":"analytics-engineering/#collecting","text":"But there's now another problem - what if there's no reliable way to differentiate those bot users? The simplest solution might be to ask the engineers that implemented the automated QA to","title":"Collecting"},{"location":"analytics-engineering/#what-analytics-engineering-is-not","text":"Building data pipelines Pulling data Predictive modelling","title":"What analytics engineering is not"},{"location":"crazy-things-I-found-in-my-code/","text":"Crazy stuff I've found in my own code It's always good to not take yourself too seriously, so I decided to go back through one of my projects from five years ago and find some of the crazy things I did, just for kicks. To be clear: this was a project that I was incredibly proud of. I remember wanting to put something on my public Github profile that really \"showed off\" what I could do. I still think publishing some work is a good thing, but at the time I probably thought that what I put on my Github profile mattered more than it actually does. The old project that I chose to go through is a collection of iPython notebooks that could scrape the results of the IAAF Track & Field world championship races from the IAAF website , and also run some analysis on the results. This was a topic close to my heart, since I had been running track for eight years through high school and college. But man, what was I thinking?! Naming variables with a single letter To be fair, I've seen this hundreds of times even from engineers paid ridiculously high salaries. But it's always a terrible idea, no matter who does it. def get_margin(g): \"\"\"Logic to extract the winning margin from a dataframe of the first and second place results in a race. Accepts a pandas.DataFrame.groupby object.\"\"\" ... In my defense, at least I chose the letter g , which is the first letter of the type of object the function accepts. That shows I was aware using the variable name x would have been less helpful, and I chose not to be as unhelpful as possible. Hardcoding resource locations df = pd.read_csv('/Users/tom/Projects/IAAF-Stats/Scripts/IAAF_Results.csv') The idea that someone else would want to run my code on a computer that wasn't my laptop must have been surprising to me. Defining functions within other functions I would like to think this was just an accident in my indentation, but it happens three times in a row in a single function. def parse_web_page(page_name): def get_athlete_details(athlete): ... def get_mark_details(mark): ... def get_country_details(country): ... page = bs(open(page_name)) ... At the time, I must have thought there was a reason to \"hide\" the inner functions, but I have no idea what I was thinking. TODOs that will never be done # To do: plotting We all have good intentions. But I have never, ever, ever seen someone fix a TODO . My TODOs from that project are 5 years old, so it's safe to say they won't be the exception. Graphs that don't really make any sense This isn't really a code issue, exactly, but it's still funny. I think what the chart above is trying to show is how close the 100m races at the IAAF championships have been, for both the men and women. But the only way to figure that out is to actually read the code that produced the graph (haha, good luck). The x-axis is the championship number, so \"8\" on the x-axis means the 8th championship since they started being held. It should probably have been \"year\" and \"location\", ie the number 0 on the x-axis should have been \"Helsinki, Finland (1983)\". The y-axis shows the percent difference between the winner's time and the runner-up's time, so lines closer to 0 mean a closer race. The last race in the dataset was an extremely close race - the winner was about 0.1% faster than the runner up, which was just a few milliseconds. In conclusion, I digress I wrote recently about how we're usually exhausted just getting the thing to do the thing we want , and we don't usually have the energy left for the simple questions, like \"what are the units on the graph?\", or \"how will someone install this?\" Looking back at my old code, I've realized that quality is often just about consistently avoiding game-ending mistakes, and I find that very encouraging. My work doesn't need to be great, it just needs to work.","title":"Crazy stuff I've found in my own code"},{"location":"crazy-things-I-found-in-my-code/#crazy-stuff-ive-found-in-my-own-code","text":"It's always good to not take yourself too seriously, so I decided to go back through one of my projects from five years ago and find some of the crazy things I did, just for kicks. To be clear: this was a project that I was incredibly proud of. I remember wanting to put something on my public Github profile that really \"showed off\" what I could do. I still think publishing some work is a good thing, but at the time I probably thought that what I put on my Github profile mattered more than it actually does. The old project that I chose to go through is a collection of iPython notebooks that could scrape the results of the IAAF Track & Field world championship races from the IAAF website , and also run some analysis on the results. This was a topic close to my heart, since I had been running track for eight years through high school and college. But man, what was I thinking?!","title":"Crazy stuff I've found in my own code"},{"location":"crazy-things-I-found-in-my-code/#naming-variables-with-a-single-letter","text":"To be fair, I've seen this hundreds of times even from engineers paid ridiculously high salaries. But it's always a terrible idea, no matter who does it. def get_margin(g): \"\"\"Logic to extract the winning margin from a dataframe of the first and second place results in a race. Accepts a pandas.DataFrame.groupby object.\"\"\" ... In my defense, at least I chose the letter g , which is the first letter of the type of object the function accepts. That shows I was aware using the variable name x would have been less helpful, and I chose not to be as unhelpful as possible.","title":"Naming variables with a single letter"},{"location":"crazy-things-I-found-in-my-code/#hardcoding-resource-locations","text":"df = pd.read_csv('/Users/tom/Projects/IAAF-Stats/Scripts/IAAF_Results.csv') The idea that someone else would want to run my code on a computer that wasn't my laptop must have been surprising to me.","title":"Hardcoding resource locations"},{"location":"crazy-things-I-found-in-my-code/#defining-functions-within-other-functions","text":"I would like to think this was just an accident in my indentation, but it happens three times in a row in a single function. def parse_web_page(page_name): def get_athlete_details(athlete): ... def get_mark_details(mark): ... def get_country_details(country): ... page = bs(open(page_name)) ... At the time, I must have thought there was a reason to \"hide\" the inner functions, but I have no idea what I was thinking.","title":"Defining functions within other functions"},{"location":"crazy-things-I-found-in-my-code/#todos-that-will-never-be-done","text":"# To do: plotting We all have good intentions. But I have never, ever, ever seen someone fix a TODO . My TODOs from that project are 5 years old, so it's safe to say they won't be the exception.","title":"TODOs that will never be done"},{"location":"crazy-things-I-found-in-my-code/#graphs-that-dont-really-make-any-sense","text":"This isn't really a code issue, exactly, but it's still funny. I think what the chart above is trying to show is how close the 100m races at the IAAF championships have been, for both the men and women. But the only way to figure that out is to actually read the code that produced the graph (haha, good luck). The x-axis is the championship number, so \"8\" on the x-axis means the 8th championship since they started being held. It should probably have been \"year\" and \"location\", ie the number 0 on the x-axis should have been \"Helsinki, Finland (1983)\". The y-axis shows the percent difference between the winner's time and the runner-up's time, so lines closer to 0 mean a closer race. The last race in the dataset was an extremely close race - the winner was about 0.1% faster than the runner up, which was just a few milliseconds.","title":"Graphs that don't really make any sense"},{"location":"crazy-things-I-found-in-my-code/#in-conclusion-i-digress","text":"I wrote recently about how we're usually exhausted just getting the thing to do the thing we want , and we don't usually have the energy left for the simple questions, like \"what are the units on the graph?\", or \"how will someone install this?\" Looking back at my old code, I've realized that quality is often just about consistently avoiding game-ending mistakes, and I find that very encouraging. My work doesn't need to be great, it just needs to work.","title":"In conclusion, I digress"},{"location":"data-lakes-vs-data-warehouses/","text":"Data Lakes vs Data Warehouses Yesterday I read through an interesting discussion on the A16Z blog, \" The Great Data Debate ,\" about whether data lakes would consume data warehouses or vice versa. I don't really have a horse in this race, but it's clear some companies and investors are making large bets. Snowflake, which produces one of the \"Big Three\" data warehouses, recently went public in the largest SaaS IPO in history. The company's former CEO Bog Muglia (who's rather outspoken) was at the discussion and said \"five years from now data is going to sit behind a SQL prompt, and SQL data warehouses will replace data lakes.\" But Andreesen Horowitz's GP Martin Casado saw it differently: \"I actually think over time you can argue that it\u2019s the data lake that ends up consuming everything, not the data warehouse.\" So I wanted to take a step back and try to understand the major differences between a data lake and a data warehouse. What are the trade-offs between the two types of technology, especially for a practitioner at a company that's not \"mega scale\" like Facebook? What's the difference between a lake and a warehouse? I'll take the definition of a Data Lake that's used in the A16Z article: Data lakes is a blurry term used by different people to mean different things, but for the purposes of this discussion, let\u2019s define data lakes as tabular data \u2013 so tables, rows and columns \u2013 stored in an open source file format, like Parquet or ORC, in a public cloud object storage, like S3 or Google Cloud storage. The key part there is public cloud object storage . If you've worked with data that lives in S3 before, you'll appreciate that S3 offers no guarantees about the data itself: it might have null values that you don't want, and there's no notion of relationships between tables defined by constructs like foreign keys. It's like storing CSVs that are parsed whenever you run a query. I'll define data warehouses a bit more directly: Data warehouses are any columnar store database - including Snowflake, Redshift, Bigquery, and even Postgres with the cstore_fdw extension. They use a relational model and can provide guarantees about the state of the data that's been inserted. So data lakes lose some of the data integrity benefits of a data warehouse, but by using a more generalized object storage, they enable running more general computations than what the data warehouse gives you, and they can also process data types that aren't supported by data warehouses, such as photos, audio, and videos. It's also worth highlighting that the definitions I've used are different than what I see used more commonly. Martin Fowler, for example, emphasizes that the data in a data lake is \"raw\" while in a data warehouse that data has been transformed into a \"single schema\" (and presumably into a format that's helpful to end-users). The data lake stores raw data, in whatever form the data source provides. There is no assumptions about the schema of the data, each data source can use whatever schema it likes... Data warehouses tend to go with the notion of a single schema for all analytics needs. - Martin Fowler, \"DataLake (2015)\" I've emphasized a different distinction between warehouses and lakes because the definition that Martin uses is a bit out of date since he wrote it back in 2015. Today, almost everyone inserts \"raw\" data into their data warehouses, rather than modifying it in any way before inserting it. That was made possible because the newer warehouses like BigQuery and Snowflake separate storage costs from compute costs, and it's no longer cost-prohibitive to store all an organization's data in the warehouse directly. So if you accept the definitions I've chosen, that data warehouses offer more guarantees about the state of the data, but less generalized computation, especially on complex data types like photos, audio, and videos, then it's clear that data lakes are only more useful than data warehouses when you have a need for those generalized computations. In other words, when you need machine learning. Will Warehouses do Machine Learning? Since the primary use case for data lakes is really just machine learning, especially on complex data types, it's worth considering whether warehouses will enable that kind of modelling in the future, and how long away that future is. Bob Muglia estimated it would be around 2025: The question is: what are the operations that actually need to be performed against data that sits in a data lake? Today anything associated with complex data, the data warehouse can\u2019t help you, and so there\u2019s a huge reason to have a data lake today. In 2025, I don\u2019t think so. Personally, I'd expect that all the warehouses offer at least some support for building models on complex data types before 2025, as BigQuery already offers BigQuery ML for building regressions, clustering, and even neural nets. You can already run code today that looks like this: create model \"product_recommender\" ( MODEL_TYPE='DNN_CLASSIFIER', ... ) So I think it's fair to say that some machine learning and modelling capabilities are coming to warehouses, and personally I'm excited for those new features. If warehouses offer machine learning, who needs a data lake? Although warehouses will support some machine learning, I'm skeptical they'll ever be able to support all the modelling requirements of every organization, especially a large enterprise where machine learning is mission critical. For example, Facebook analyzes every image that gets uploaded for content that violates its community guidelines, while YouTube does the same for every video. Companies like Rev have also built their entire product around transcribing audio to text, and likely do some of that automatically through models. Those models will always need to be built on a custom \"in-house platform,\" because the companies require unique customizations over their models, and it's not cost effective for vendors to support those unique customizations when they're only relevant for a single customer. But in my view, the need to run such sophisticated machine learning is comparatively niche. Most companies simply have no user-generated content they could even use, so building machine learning on complex data types like photos, audio, or video isn't relevant for them. In the small number of cases where companies do have a genuine need for machine learning, I expect that the machine learning supported by the data warehouses will often be sufficient. In the even smaller number of cases where companies need specialized machine learning that isn't supported by the warehouse, companies will usually be satisfied by loading just the data they need into an \"ML environment,\" such as a simple Python Notebook, and building the models there. While that would mean some data duplication between environments, in my view it's a better alternative than supporting an entire data lake. So I expect most organizations will adopt warehouses over lakes, while only the huge enterprises with mission-critical models will find it worth the cost to maintain their own data lakes and associated machine learning infrastructure.","title":"Data Lakes vs Data Warehouses"},{"location":"data-lakes-vs-data-warehouses/#data-lakes-vs-data-warehouses","text":"Yesterday I read through an interesting discussion on the A16Z blog, \" The Great Data Debate ,\" about whether data lakes would consume data warehouses or vice versa. I don't really have a horse in this race, but it's clear some companies and investors are making large bets. Snowflake, which produces one of the \"Big Three\" data warehouses, recently went public in the largest SaaS IPO in history. The company's former CEO Bog Muglia (who's rather outspoken) was at the discussion and said \"five years from now data is going to sit behind a SQL prompt, and SQL data warehouses will replace data lakes.\" But Andreesen Horowitz's GP Martin Casado saw it differently: \"I actually think over time you can argue that it\u2019s the data lake that ends up consuming everything, not the data warehouse.\" So I wanted to take a step back and try to understand the major differences between a data lake and a data warehouse. What are the trade-offs between the two types of technology, especially for a practitioner at a company that's not \"mega scale\" like Facebook?","title":"Data Lakes vs Data Warehouses"},{"location":"data-lakes-vs-data-warehouses/#whats-the-difference-between-a-lake-and-a-warehouse","text":"I'll take the definition of a Data Lake that's used in the A16Z article: Data lakes is a blurry term used by different people to mean different things, but for the purposes of this discussion, let\u2019s define data lakes as tabular data \u2013 so tables, rows and columns \u2013 stored in an open source file format, like Parquet or ORC, in a public cloud object storage, like S3 or Google Cloud storage. The key part there is public cloud object storage . If you've worked with data that lives in S3 before, you'll appreciate that S3 offers no guarantees about the data itself: it might have null values that you don't want, and there's no notion of relationships between tables defined by constructs like foreign keys. It's like storing CSVs that are parsed whenever you run a query. I'll define data warehouses a bit more directly: Data warehouses are any columnar store database - including Snowflake, Redshift, Bigquery, and even Postgres with the cstore_fdw extension. They use a relational model and can provide guarantees about the state of the data that's been inserted. So data lakes lose some of the data integrity benefits of a data warehouse, but by using a more generalized object storage, they enable running more general computations than what the data warehouse gives you, and they can also process data types that aren't supported by data warehouses, such as photos, audio, and videos. It's also worth highlighting that the definitions I've used are different than what I see used more commonly. Martin Fowler, for example, emphasizes that the data in a data lake is \"raw\" while in a data warehouse that data has been transformed into a \"single schema\" (and presumably into a format that's helpful to end-users). The data lake stores raw data, in whatever form the data source provides. There is no assumptions about the schema of the data, each data source can use whatever schema it likes... Data warehouses tend to go with the notion of a single schema for all analytics needs. - Martin Fowler, \"DataLake (2015)\" I've emphasized a different distinction between warehouses and lakes because the definition that Martin uses is a bit out of date since he wrote it back in 2015. Today, almost everyone inserts \"raw\" data into their data warehouses, rather than modifying it in any way before inserting it. That was made possible because the newer warehouses like BigQuery and Snowflake separate storage costs from compute costs, and it's no longer cost-prohibitive to store all an organization's data in the warehouse directly. So if you accept the definitions I've chosen, that data warehouses offer more guarantees about the state of the data, but less generalized computation, especially on complex data types like photos, audio, and videos, then it's clear that data lakes are only more useful than data warehouses when you have a need for those generalized computations. In other words, when you need machine learning.","title":"What's the difference between a lake and a warehouse?"},{"location":"data-lakes-vs-data-warehouses/#will-warehouses-do-machine-learning","text":"Since the primary use case for data lakes is really just machine learning, especially on complex data types, it's worth considering whether warehouses will enable that kind of modelling in the future, and how long away that future is. Bob Muglia estimated it would be around 2025: The question is: what are the operations that actually need to be performed against data that sits in a data lake? Today anything associated with complex data, the data warehouse can\u2019t help you, and so there\u2019s a huge reason to have a data lake today. In 2025, I don\u2019t think so. Personally, I'd expect that all the warehouses offer at least some support for building models on complex data types before 2025, as BigQuery already offers BigQuery ML for building regressions, clustering, and even neural nets. You can already run code today that looks like this: create model \"product_recommender\" ( MODEL_TYPE='DNN_CLASSIFIER', ... ) So I think it's fair to say that some machine learning and modelling capabilities are coming to warehouses, and personally I'm excited for those new features.","title":"Will Warehouses do Machine Learning?"},{"location":"data-lakes-vs-data-warehouses/#if-warehouses-offer-machine-learning-who-needs-a-data-lake","text":"Although warehouses will support some machine learning, I'm skeptical they'll ever be able to support all the modelling requirements of every organization, especially a large enterprise where machine learning is mission critical. For example, Facebook analyzes every image that gets uploaded for content that violates its community guidelines, while YouTube does the same for every video. Companies like Rev have also built their entire product around transcribing audio to text, and likely do some of that automatically through models. Those models will always need to be built on a custom \"in-house platform,\" because the companies require unique customizations over their models, and it's not cost effective for vendors to support those unique customizations when they're only relevant for a single customer. But in my view, the need to run such sophisticated machine learning is comparatively niche. Most companies simply have no user-generated content they could even use, so building machine learning on complex data types like photos, audio, or video isn't relevant for them. In the small number of cases where companies do have a genuine need for machine learning, I expect that the machine learning supported by the data warehouses will often be sufficient. In the even smaller number of cases where companies need specialized machine learning that isn't supported by the warehouse, companies will usually be satisfied by loading just the data they need into an \"ML environment,\" such as a simple Python Notebook, and building the models there. While that would mean some data duplication between environments, in my view it's a better alternative than supporting an entire data lake. So I expect most organizations will adopt warehouses over lakes, while only the huge enterprises with mission-critical models will find it worth the cost to maintain their own data lakes and associated machine learning infrastructure.","title":"If warehouses offer machine learning, who needs a data lake?"},{"location":"dim-and-fact-tables/","text":"Dimension tables and Fact tables A common question that analysts have when they start working with a data warehouse is: what's the difference between a fact table and a dimension table? Those labels often appear as abbreviations prefixed on a table's name, like dim_users or fct_clicks , or sometimes just d_users or f_clicks . So if you ever want to create your own table in the warehouse, you'll need to decide which label to use. Unfortunately, Google doesn't provide much help with that decision. Here's what the first results gives: A cryptic definition: The fact table contains business facts (or measures), and foreign keys which refer to candidate keys (normally primary keys) in the dimension tables. Contrary to fact tables, dimension tables contain descriptive attributes (or fields) that are typically textual fields (or discrete numbers that behave like text). To be honest, I don't really understand that definition or how to apply it. So I thought it would be helpful to provide an alternative explanation about fact and dimension tables which is more practical, and which always gives you the right answer. The short answer is: a fact table contains records which will never change. Sometimes the data in a fact table \"immutable\" or \"static.\" That means that a fact table will never have someone run an UPDATE query on any of its records. So if you look at a specific record in a fact table, you are gauranteed to get back the same result every time, no matter whether you run the query today or a year from now. # Always returns the same result, because # f_clicks is a fact table select * from f_clicks where id=1 Records can be inserted into fact tables, but once a record has been inserted, it can never be updated. Records can also never be deleted from fact tables. A dimension table, however, offers no gaurantee that the data it contains will never change. For example, you might be interested in the time that a user from the dim_users table last logged in. select last_login_at from dim_users where id=1 '2020-09-15 12:34:56.987' But maybe that user will log in again at a later date. Running that same query would then return a different result - the data for that record has changed. select last_login_at from dim_users where id=1 '2020-09-24 09:08:07.543' Sometimes people call the data in a dimension table \"mutable\" or \"dynamic,\" in contrast to the \"immutable\" or \"static\" data in a fact table. So to decide if your table is a dimension table or a fact table, you need to answer the question: will the data in my table ever change? If you're confident the data will never change, then it's a fact table. Common fact tables include historical records of events, like when a page was viewed, or when an order was placed. But if the data in your table will change, then it's a dim table. Common dim tables include information about a user's account, or the contents of an article on their blog.","title":"Dimension tables and Fact tables"},{"location":"dim-and-fact-tables/#dimension-tables-and-fact-tables","text":"A common question that analysts have when they start working with a data warehouse is: what's the difference between a fact table and a dimension table? Those labels often appear as abbreviations prefixed on a table's name, like dim_users or fct_clicks , or sometimes just d_users or f_clicks . So if you ever want to create your own table in the warehouse, you'll need to decide which label to use. Unfortunately, Google doesn't provide much help with that decision. Here's what the first results gives: A cryptic definition: The fact table contains business facts (or measures), and foreign keys which refer to candidate keys (normally primary keys) in the dimension tables. Contrary to fact tables, dimension tables contain descriptive attributes (or fields) that are typically textual fields (or discrete numbers that behave like text). To be honest, I don't really understand that definition or how to apply it. So I thought it would be helpful to provide an alternative explanation about fact and dimension tables which is more practical, and which always gives you the right answer. The short answer is: a fact table contains records which will never change. Sometimes the data in a fact table \"immutable\" or \"static.\" That means that a fact table will never have someone run an UPDATE query on any of its records. So if you look at a specific record in a fact table, you are gauranteed to get back the same result every time, no matter whether you run the query today or a year from now. # Always returns the same result, because # f_clicks is a fact table select * from f_clicks where id=1 Records can be inserted into fact tables, but once a record has been inserted, it can never be updated. Records can also never be deleted from fact tables. A dimension table, however, offers no gaurantee that the data it contains will never change. For example, you might be interested in the time that a user from the dim_users table last logged in. select last_login_at from dim_users where id=1 '2020-09-15 12:34:56.987' But maybe that user will log in again at a later date. Running that same query would then return a different result - the data for that record has changed. select last_login_at from dim_users where id=1 '2020-09-24 09:08:07.543' Sometimes people call the data in a dimension table \"mutable\" or \"dynamic,\" in contrast to the \"immutable\" or \"static\" data in a fact table. So to decide if your table is a dimension table or a fact table, you need to answer the question: will the data in my table ever change? If you're confident the data will never change, then it's a fact table. Common fact tables include historical records of events, like when a page was viewed, or when an order was placed. But if the data in your table will change, then it's a dim table. Common dim tables include information about a user's account, or the contents of an article on their blog.","title":"Dimension tables and Fact tables"},{"location":"do-it-manually/","text":"Do it manually The stackoverflow surveys project. I spent a ridiculous amount of time trying to make the \"data cleaning\" process fully automated, even though I knew I'd only have to run it one time. Trying to do everything fully automated can be good, because it \"proves fidelity,\" but its okay to relax a little bit your standards for \"reproducibility.\" Requiring everyone to take the exact same download link and reproduce the project from those is just too much. I did discover some useful new tools, namely csvkit and pgloader. I avoided the manual work because it would have required installing a new tool (editing a CSV on vscode) and VSCode itself is kind of awkwaard to edit CSV files in. Having the right tool makes it much easier to jump into the type of work. The first time I tried doing this project, I wanted to automate everything from the links themselves. THe second time, it was from just the .csv files. Once I had the CSV files, it was more efficient to make progress by continueing rather than starting over. That's the symptom of effective work.","title":"Do it manually"},{"location":"do-it-manually/#do-it-manually","text":"The stackoverflow surveys project. I spent a ridiculous amount of time trying to make the \"data cleaning\" process fully automated, even though I knew I'd only have to run it one time. Trying to do everything fully automated can be good, because it \"proves fidelity,\" but its okay to relax a little bit your standards for \"reproducibility.\" Requiring everyone to take the exact same download link and reproduce the project from those is just too much. I did discover some useful new tools, namely csvkit and pgloader. I avoided the manual work because it would have required installing a new tool (editing a CSV on vscode) and VSCode itself is kind of awkwaard to edit CSV files in. Having the right tool makes it much easier to jump into the type of work. The first time I tried doing this project, I wanted to automate everything from the links themselves. THe second time, it was from just the .csv files. Once I had the CSV files, it was more efficient to make progress by continueing rather than starting over. That's the symptom of effective work.","title":"Do it manually"},{"location":"favorites/","text":"This is water By David Foster Wallace. There's a quote I've heard - but unfortunately I can't find the attribution - and it goes like: If you don't like what you see, change how you're looking at it. But most of the time we don't reflect on how we're choosing to see the world, or if we do think about it, it's only to justify the viewpoints we've already chosen. 95th percentile isn't that good By Dan Luu. If being aware of the choices you make about what to think is rare, putting in the practice necessary to change those thoughts is rarer Having values is not the same as living them. J-School Confidential By Michael Lewis. There's a kind of fake-intellectual exclusivity around \"how to think\" however. A kind of bizarre language that gets used, \"social capital\". Sapiens: A Brief History of Humankind By Yuval Noah Harari. Monkeys with a plan. The Era of Visual Studio Code By Roben Kleene. Choosing tools is an important decision. Tools are where we get leverage.","title":"Favorites"},{"location":"favorites/#this-is-water","text":"By David Foster Wallace. There's a quote I've heard - but unfortunately I can't find the attribution - and it goes like: If you don't like what you see, change how you're looking at it. But most of the time we don't reflect on how we're choosing to see the world, or if we do think about it, it's only to justify the viewpoints we've already chosen.","title":"This is water"},{"location":"favorites/#95th-percentile-isnt-that-good","text":"By Dan Luu. If being aware of the choices you make about what to think is rare, putting in the practice necessary to change those thoughts is rarer Having values is not the same as living them.","title":"95th percentile isn't that good"},{"location":"favorites/#j-school-confidential","text":"By Michael Lewis. There's a kind of fake-intellectual exclusivity around \"how to think\" however. A kind of bizarre language that gets used, \"social capital\".","title":"J-School Confidential"},{"location":"favorites/#sapiens-a-brief-history-of-humankind","text":"By Yuval Noah Harari. Monkeys with a plan.","title":"Sapiens: A Brief History of Humankind"},{"location":"favorites/#the-era-of-visual-studio-code","text":"By Roben Kleene. Choosing tools is an important decision. Tools are where we get leverage.","title":"The Era of Visual Studio Code"},{"location":"generating-cohorts-for-an-experiment/","text":"How to allocate users to a random cohort for an A/B test I create https://api.getcohorts.com because randomly allocating users to a cohort can be tricky. Ideally, a function that allocates a user to a random cohort will meet the following requirements: The function is idempotent, so the same user will be allocated to the same cohort every time. This requirement is necessary because it's likely that the user will be exposed to your experiment multiple times. For example, you might be testing a new image on your website's homepage, and the user will return to the homepage several times. You'll want to make sure they see the same experience each time. A simple way to implement an idempotent function would be to randomly assign the user to a cohort, and then cache its result. If the same user ever needs to be assigned to the cohort again, then the result from the cache is returned. # requires python 3.8+ import redis import random from typing import List db = redis.Redis() def allocate(user_id: int, experiment: str, cohorts: List[str]): key = f'experiment-{experiment}-{user_id}' if (cohort := db.get(key)) is not None: return cohort.decode('utf8') else: index = int(random.random() * len(cohorts)) cohort = cohorts[index] db.set(key, cohort) return cohort This methodology works very well, but it has the downside of requiring a database. The function is stateless, so there is no need to cache any results The function is accessible from as many locations as possible The function can be applied in batches on a datawarehouse","title":"How to allocate users to a random cohort for an A/B test"},{"location":"generating-cohorts-for-an-experiment/#how-to-allocate-users-to-a-random-cohort-for-an-ab-test","text":"I create https://api.getcohorts.com because randomly allocating users to a cohort can be tricky. Ideally, a function that allocates a user to a random cohort will meet the following requirements:","title":"How to allocate users to a random cohort for an A/B test"},{"location":"generating-cohorts-for-an-experiment/#the-function-is-idempotent-so-the-same-user-will-be-allocated-to-the-same-cohort-every-time","text":"This requirement is necessary because it's likely that the user will be exposed to your experiment multiple times. For example, you might be testing a new image on your website's homepage, and the user will return to the homepage several times. You'll want to make sure they see the same experience each time. A simple way to implement an idempotent function would be to randomly assign the user to a cohort, and then cache its result. If the same user ever needs to be assigned to the cohort again, then the result from the cache is returned. # requires python 3.8+ import redis import random from typing import List db = redis.Redis() def allocate(user_id: int, experiment: str, cohorts: List[str]): key = f'experiment-{experiment}-{user_id}' if (cohort := db.get(key)) is not None: return cohort.decode('utf8') else: index = int(random.random() * len(cohorts)) cohort = cohorts[index] db.set(key, cohort) return cohort This methodology works very well, but it has the downside of requiring a database.","title":"The function is idempotent, so the same user will be allocated to the same cohort every time."},{"location":"generating-cohorts-for-an-experiment/#the-function-is-stateless-so-there-is-no-need-to-cache-any-results","text":"","title":"The function is stateless, so there is no need to cache any results"},{"location":"generating-cohorts-for-an-experiment/#the-function-is-accessible-from-as-many-locations-as-possible","text":"","title":"The function is accessible from as many locations as possible"},{"location":"generating-cohorts-for-an-experiment/#the-function-can-be-applied-in-batches-on-a-datawarehouse","text":"","title":"The function can be applied in batches on a datawarehouse"},{"location":"github-announces-codespaces-web-based-ide/","text":"Github announces Codespaces, a web-based IDE On May 8th at Github Satellite, the company announced the release of its new product Codespaces, a web-based IDE that integrates with Github repositories. Since Microsoft acquired Github in late 2018, Codespaces is the third major new product that Github has announced, along with their continuous integration service Github Actions and their package management service Github Packages. Web-based IDE's like Codespaces can have significant benefits for developers. By standardizing the development environment for a project, along with enabling creation of those environments with a single click, a web-based IDE can save a significant amount of time getting started with a new development project, and reduce errors during development that are caused by different environments. In short, they help solve the \"works on my machine\" problem that's frequently encountered during development. Unlike non web-based IDEs such as Eclipse, Pycharm, or Intellij, or Github's own Atom, which run on a developer's laptop, a web-based IDE runs in a web browser and is connected to a remote virtual machine, similar to a remote desktop. The configuration of the remote virtual machine is managed in the same repository as the application's code itself, so two developers that use a web-based IDE can be guaranteed their virtual machines have the same configuration. If they used a non web-based IDE, they could never be certain that their two different laptops have the exact same configuration, leading to the \"works on my machine\" problem. It's likely that Github's Codespaces borrows significantly from Microsoft's Visual Studio Codespaces, and in fact the company renamed Visual Studio Online to Visual Studio Codespaces on the same day that Github announced their new product. However Github Codespaces has a different look and feel from Microsoft's Visual Studio, and will likely offer tighter integrations with Github repositories. The invention of web-based IDEs is nothing new, with AWS's Cloud 9 released in late 2017, and open-source \"IDE's\" like the Jupyter Notebook have been available since the early 2010's. But the announcement of Github Codespaces is still significant, since it means that open-source projects which rely on selling a web-based development environment will become less differentiated from their zero-cost self-hosted versions. As Github is by far the largest host of open-source projects in the world, their ability to offer \"1-click\" provisioning of a web-based development environment for projects like DBT, Airflow, and even Anaconda, will likely make it cheaper for companies to in-source the for-profit SaaS products provided by vendors funding the developers of those open-source projects.","title":"Github announces Codespaces, a web-based IDE"},{"location":"github-announces-codespaces-web-based-ide/#github-announces-codespaces-a-web-based-ide","text":"On May 8th at Github Satellite, the company announced the release of its new product Codespaces, a web-based IDE that integrates with Github repositories. Since Microsoft acquired Github in late 2018, Codespaces is the third major new product that Github has announced, along with their continuous integration service Github Actions and their package management service Github Packages. Web-based IDE's like Codespaces can have significant benefits for developers. By standardizing the development environment for a project, along with enabling creation of those environments with a single click, a web-based IDE can save a significant amount of time getting started with a new development project, and reduce errors during development that are caused by different environments. In short, they help solve the \"works on my machine\" problem that's frequently encountered during development. Unlike non web-based IDEs such as Eclipse, Pycharm, or Intellij, or Github's own Atom, which run on a developer's laptop, a web-based IDE runs in a web browser and is connected to a remote virtual machine, similar to a remote desktop. The configuration of the remote virtual machine is managed in the same repository as the application's code itself, so two developers that use a web-based IDE can be guaranteed their virtual machines have the same configuration. If they used a non web-based IDE, they could never be certain that their two different laptops have the exact same configuration, leading to the \"works on my machine\" problem. It's likely that Github's Codespaces borrows significantly from Microsoft's Visual Studio Codespaces, and in fact the company renamed Visual Studio Online to Visual Studio Codespaces on the same day that Github announced their new product. However Github Codespaces has a different look and feel from Microsoft's Visual Studio, and will likely offer tighter integrations with Github repositories. The invention of web-based IDEs is nothing new, with AWS's Cloud 9 released in late 2017, and open-source \"IDE's\" like the Jupyter Notebook have been available since the early 2010's. But the announcement of Github Codespaces is still significant, since it means that open-source projects which rely on selling a web-based development environment will become less differentiated from their zero-cost self-hosted versions. As Github is by far the largest host of open-source projects in the world, their ability to offer \"1-click\" provisioning of a web-based development environment for projects like DBT, Airflow, and even Anaconda, will likely make it cheaper for companies to in-source the for-profit SaaS products provided by vendors funding the developers of those open-source projects.","title":"Github announces Codespaces, a web-based IDE"},{"location":"github-codespaces/","text":"First thoughts on Github Codespaces I've been looking forward to trying out Github Codespaces since it was announced at Github's developer conference back in May, and was excited to get invited to the beta a few weeks ago. Codespaces are an \"on-demand\" development server that's customized for each project through a devcontainer.json file. The on-demand devservers also integrate nicely with VS Code, which has become the most popular editor and may stay that way for a long time , and is my personal editor of choice. Using on-demand environments for a development project is very compelling: getting started developing on a project will be much simpler, and the painful \"works on my machine\" issue is less likely to occur. On-demand devservers are also much cheaper than dedicated devservers. We're switching to on-demand devservers at Facebook for those reasons, and I expect other companies are doing the same. I've tried out other on-demand environnments, including AWS's Cloud9, but never stuck with them. But since I use VS Code, and most of my projects live on Github, I was keen to see if Codespaces could replace doing development locally on my Macbook. The short answer is: mostly. I've been doing my blog's development on Codespaces for the last two weeks, and it's gone really well, especially for just editing some CSS and markdown. But another project I've been working on, which involves some datasets that are about 100MB, didn't go as well: opening those files kept causing VS Code to hang and requiring me to restart the devserver. Those issues came up because the beta version of their on-demand servers are pretty lite, with just 2 cores, 4GB of ram and 64GB of disk, although the pricing they've announced for the full release includes machines with up to 8 cores and 16 GB of ram. But for app development projects that don't have lots of heavy data, there's already several great features available in the beta version which make Github's on-demand devservers a compelling alternative to developing locally, especially if you're already using VS Code or Github. First, it's really easy to connect to your on-demand devserver straight from VSCode. Once you've created a Codespace devserver from the web UI, you can just use the command \"Open in VS Code,\" and it opens your VS Code desktop app, prompts you to install all the relevant Codespace extensions for VS Code, and connects to your remote server flawlessly. Using the VS Code desktop app is also completely optional, as the web based editor is still quite good, although it won't have all your local VS Code features like custom keyboard shortcuts. You can also connect to an existing dev server seamlessly from your VSCode desktop app. From VS Code, just run the command \"Connect to Codespaces\" and select the environment you want. If your codespace was sleeping, it will wake up automatically and be ready in about a minute. Second, viewing the web apps you're developing on the remote devserver through your browser is also straightforward. I was nervous about this, because having configured my own remote dev environments on AWS before, it's tricky to get features like live-reloading working correctly. But I was able to run the VS Code command \"Forward a Port\", and then opened localhost in my browser. Live reload worked perfectly without any configuration necessary. What a time to be alive! Third, it's also pretty easy to personalize you devserver environment. You'll need to set up a public dotfiles repo on your Github account, which will automatically run an install.sh script when you create a new codespace. I've used this to install a few packages that I use regularly, like changing the terminal's shell and prompt. It's a little confusing though that the install script will run after you can access the workspace, so the first time you're on, you'll be be curious why the commands you've installed aren't available. You can also watch the progress of your install.sh script in the web page of the Codespace's \"creation log\". So VS Code makes it really easy to get a web-app development project up and running, and personalize your environment. The next logical question is to ask about deploying your project: how do devservers handle secrets, like SSH keys for for connecting to a \"production\" server? Secret's are currently not supported in Codespaces, but they're on the roadmap . Without secrets, it's not really possible for Codespaces to completely take over a \"dev environment,\" since those environments typically need to connect to production services in some way, and not just modify an application locally. But it's not a big deal to add those credentials to your environment, since the state of the workspace will be saved even when you shut it down. Another wonderful feature is that the on-demand devserver is auto stopped once it's been idle for 30 minutes. You'll still pay the storage cost associated with your instance while it's alseep, and I'd estimate that their \"basic\" type will probably run about $15 per month all in, depending on how much development you do. There's more details of their pricing here . Github hasn't announced an official release date for Codespaces yet, but it's probably going to be some time in 2021. I'm really curious to see how companies move to on-demand devservers, and whether open-source projects start adding those devcontainer.json files. These environments will also make teaching programming so much better - I'd wager some schools will start moving their course materials over to Github to take advantage of the simplified environment set up. It's really amazing how much developer tools have improved in just the last few years, with free CI/CD, editors like VSCode, and now on-demand devservers. On-demand devservers might just be the biggest improvement yet.","title":"First thoughts on Github Codespaces"},{"location":"github-codespaces/#first-thoughts-on-github-codespaces","text":"I've been looking forward to trying out Github Codespaces since it was announced at Github's developer conference back in May, and was excited to get invited to the beta a few weeks ago. Codespaces are an \"on-demand\" development server that's customized for each project through a devcontainer.json file. The on-demand devservers also integrate nicely with VS Code, which has become the most popular editor and may stay that way for a long time , and is my personal editor of choice. Using on-demand environments for a development project is very compelling: getting started developing on a project will be much simpler, and the painful \"works on my machine\" issue is less likely to occur. On-demand devservers are also much cheaper than dedicated devservers. We're switching to on-demand devservers at Facebook for those reasons, and I expect other companies are doing the same. I've tried out other on-demand environnments, including AWS's Cloud9, but never stuck with them. But since I use VS Code, and most of my projects live on Github, I was keen to see if Codespaces could replace doing development locally on my Macbook. The short answer is: mostly. I've been doing my blog's development on Codespaces for the last two weeks, and it's gone really well, especially for just editing some CSS and markdown. But another project I've been working on, which involves some datasets that are about 100MB, didn't go as well: opening those files kept causing VS Code to hang and requiring me to restart the devserver. Those issues came up because the beta version of their on-demand servers are pretty lite, with just 2 cores, 4GB of ram and 64GB of disk, although the pricing they've announced for the full release includes machines with up to 8 cores and 16 GB of ram. But for app development projects that don't have lots of heavy data, there's already several great features available in the beta version which make Github's on-demand devservers a compelling alternative to developing locally, especially if you're already using VS Code or Github. First, it's really easy to connect to your on-demand devserver straight from VSCode. Once you've created a Codespace devserver from the web UI, you can just use the command \"Open in VS Code,\" and it opens your VS Code desktop app, prompts you to install all the relevant Codespace extensions for VS Code, and connects to your remote server flawlessly. Using the VS Code desktop app is also completely optional, as the web based editor is still quite good, although it won't have all your local VS Code features like custom keyboard shortcuts. You can also connect to an existing dev server seamlessly from your VSCode desktop app. From VS Code, just run the command \"Connect to Codespaces\" and select the environment you want. If your codespace was sleeping, it will wake up automatically and be ready in about a minute. Second, viewing the web apps you're developing on the remote devserver through your browser is also straightforward. I was nervous about this, because having configured my own remote dev environments on AWS before, it's tricky to get features like live-reloading working correctly. But I was able to run the VS Code command \"Forward a Port\", and then opened localhost in my browser. Live reload worked perfectly without any configuration necessary. What a time to be alive! Third, it's also pretty easy to personalize you devserver environment. You'll need to set up a public dotfiles repo on your Github account, which will automatically run an install.sh script when you create a new codespace. I've used this to install a few packages that I use regularly, like changing the terminal's shell and prompt. It's a little confusing though that the install script will run after you can access the workspace, so the first time you're on, you'll be be curious why the commands you've installed aren't available. You can also watch the progress of your install.sh script in the web page of the Codespace's \"creation log\". So VS Code makes it really easy to get a web-app development project up and running, and personalize your environment. The next logical question is to ask about deploying your project: how do devservers handle secrets, like SSH keys for for connecting to a \"production\" server? Secret's are currently not supported in Codespaces, but they're on the roadmap . Without secrets, it's not really possible for Codespaces to completely take over a \"dev environment,\" since those environments typically need to connect to production services in some way, and not just modify an application locally. But it's not a big deal to add those credentials to your environment, since the state of the workspace will be saved even when you shut it down. Another wonderful feature is that the on-demand devserver is auto stopped once it's been idle for 30 minutes. You'll still pay the storage cost associated with your instance while it's alseep, and I'd estimate that their \"basic\" type will probably run about $15 per month all in, depending on how much development you do. There's more details of their pricing here . Github hasn't announced an official release date for Codespaces yet, but it's probably going to be some time in 2021. I'm really curious to see how companies move to on-demand devservers, and whether open-source projects start adding those devcontainer.json files. These environments will also make teaching programming so much better - I'd wager some schools will start moving their course materials over to Github to take advantage of the simplified environment set up. It's really amazing how much developer tools have improved in just the last few years, with free CI/CD, editors like VSCode, and now on-demand devservers. On-demand devservers might just be the biggest improvement yet.","title":"First thoughts on Github Codespaces"},{"location":"giving-feedback-is-hard/","text":"Giving feedback is hard Helping other people with their work is hard: you need to be patient, encouraging, and able to contribute something that's actually constructive. Your feedback will also fail to land unless your partner finishes the session feeling confident, which means you need to find the right moment to say - \"and I want to know how it goes afterwards\" - and it's hard to time that moment to coincide with exactly the end of a 30 minute one-on-one. You also have to read the other person affectively: some people might interpret any \"confidence building\" you try do as belittling, while others may be completely uninterested in the constructive part, and really they're only interested in the \"confidence building\" part by itself. This is all made even harder when the person asking for help hasn't practiced the best way to receive that help either. Sometimes the person may genuinely want your help, but they might feel argumentative, and their responses might turn you off the conversation because you'll feel you're not having any effect. So I wanted to write this article as a kind of feedback for myself about how I'm doing feedback. Listen for what they're asking for Is it confidence, or is it constructive? Do you have the kind of relationship where they want to just \"cut to the chase.\" Understand where they're at in the process Is it late stages or early on, during their research? Try to understand where they're at, so that you can tailer your comments to their situation Don't race to the finish line We want to finish every session with a complete, coherent plan about what to do next. Good luck with that!","title":"Giving feedback is hard"},{"location":"giving-feedback-is-hard/#giving-feedback-is-hard","text":"Helping other people with their work is hard: you need to be patient, encouraging, and able to contribute something that's actually constructive. Your feedback will also fail to land unless your partner finishes the session feeling confident, which means you need to find the right moment to say - \"and I want to know how it goes afterwards\" - and it's hard to time that moment to coincide with exactly the end of a 30 minute one-on-one. You also have to read the other person affectively: some people might interpret any \"confidence building\" you try do as belittling, while others may be completely uninterested in the constructive part, and really they're only interested in the \"confidence building\" part by itself. This is all made even harder when the person asking for help hasn't practiced the best way to receive that help either. Sometimes the person may genuinely want your help, but they might feel argumentative, and their responses might turn you off the conversation because you'll feel you're not having any effect. So I wanted to write this article as a kind of feedback for myself about how I'm doing feedback.","title":"Giving feedback is hard"},{"location":"giving-feedback-is-hard/#listen-for-what-theyre-asking-for","text":"Is it confidence, or is it constructive? Do you have the kind of relationship where they want to just \"cut to the chase.\"","title":"Listen for what they're asking for"},{"location":"giving-feedback-is-hard/#understand-where-theyre-at-in-the-process","text":"Is it late stages or early on, during their research? Try to understand where they're at, so that you can tailer your comments to their situation","title":"Understand where they're at in the process"},{"location":"giving-feedback-is-hard/#dont-race-to-the-finish-line","text":"We want to finish every session with a complete, coherent plan about what to do next. Good luck with that!","title":"Don't race to the finish line"},{"location":"how-to-find-a-data-science-job-at-a-startup/","text":"How to find a Data Science job at a Start-Up Getting a job interview for a Data Science role at a Start-Up is challenging: since Start-Up\u2019s are relatively small, their job postings are harder to find, and their processes are less formal than more established companies. In my own experience applying to Start-Ups, I was eventually able to figure out a system that worked very well for me personally. The system itself is pretty simple: use Google to search the HR sites that start-ups use, like jobs.lever.co or greenhouse.io, then find the email address of the recruiter or manager using a free service like Clearbit, and then use a templated email to contact the recruiter directly and ask for an interview. I've outlined the process in more details below, including the exact searches you can use on Google, how to find the recruiter's email, and a template you can try for yourself. Using this system, I was able to solicit multiple Data Science interviews with Start-Up companies in San Francisco that I was genuinely interested in. Best of all, it was much more effective than submitting my resume through online job boards like Indeed or Monster. If you're finding that applying through job boards isn't working, try giving this system a shot. It only takes an hour or so to try it out. Step 1: Build a list of prospective job openings The most effective way I\u2019ve discovered to do this is to use google to search specific sites, using a query like the following. site:jobs.lever.co data scientist San Francisco Note how the use of site:jobs.lever.co filters Google\u2019s results to only the pages on that site, which happens to be all of the job postings on Lever. Note: Lever is a candidate tracking system that\u2019s popular among SF tech companies. Lever is very different than sites like Monster or Indeed because it isn\u2019t a listing site. I\u2019ve also had good results using the site boards.greenhouse.io. You can also use Google\u2019s search tools to only show new listings from the last month, which helps ensure the postings aren\u2019t \u201cstale.\u201d Each result will take you to a job description, and they\u2019ll include the company\u2019s logo with a link to the company\u2019s website. You should be able to find 10\u201320 interesting startups in about an hour or so by using this method and adjusting the titles you use. The postings on Lever allow you to apply directly, but I wouldn\u2019t recommend it. Instead, you should find the email address of the Start Up\u2019s recruiter and contact them directly. Step 2: email the start up\u2019s recruiter directly My experience has been that direct emails to a recruiter work much more effectively than submitting to the job posting. It\u2019s also pretty easy to find the recruiter\u2019s email once you know the company\u2019s domain name. First, use LinkedIn to look up the list of that company\u2019s employees. Search for someone that has \u201crecruiter\u201d or a similar title. Their email is almost always the following: firstname@domain.com. Ie if the company\u2019s site was polleverywhere.com and the recruiter was named Tom Waterman, you can just try tom@polleverywhere.com. (If you\u2019re applying to bigger companies, this method is less likely to work). To validate the email address, you can use http://clearbit.com. Their free product allows you to \u201cenrich\u201d an email address, and it also works for checking that the email address you\u2019re using is the right one. (Note: I am not affiliated with Clearbit in any way). But what should you do if the company doesn\u2019t have a recruiter? That\u2019s not uncommon at Start Ups. The next best person to look for is a manager on the team you\u2019re applying to, or even a founder. You might feel nervous about emailing the founders directly, but I promise they won\u2019t find it rude. If anything, they tend to appreciate the initiative. Now that you\u2019ve got the recruiter\u2019s email, you guessed it \u2014 you\u2019re going to have to email them. Step 3: explicitly ask the recruiter for a phone call You might not be sure how to email the recruiter to ask for a phone call. In my experience, the best way is to keep the email extremely brief, attach your resume, and include an explicit request for a phone call. Don't include a wall of text explaining why you\u2019re applying, your professional history, or anything else. Definitely don't address the letter using \u201csir\u201d or \"madam.\" Just state the reason for your email and the action you\u2019re asking them to take. Here\u2019s a template that has worked well for me: Hi [Recruiter first name], I\u2019m reaching out about the Data Science job posting I found on [Company\u2019s website]. Are you free for a short call to discuss whether the role could be a good fit? I\u2019m hoping to learn more about the team\u2019s current projects, and whether my skills and experience could be a good match. About me: Currently a data scientist at Facebook Previously a product analyst for a ~50 person SaaS company Previously an analyst for consulting company doing mainly analytics projects I\u2019ve also attached a resume with more details of my project experience and relevant skills Thanks for your time! Tom Step 3.5: follow up if you don\u2019t get a response It sucks to have your email ignored, but it will happen. Sometimes the recruiter is ignoring you because the role has been filled, or they don\u2019t think you\u2019re a good candidate, or maybe they were just on vacation. Don\u2019t take it personally \u2014 I\u2019m sure you\u2019ve also not replied to emails, too. Whatever the reason for not getting an email back, sending a follow up email is guaranteed to increase your response rates. I always send a follow up, even if it hurts my pride to revisit the fact that I got ignored in the first place. This system has worked really well for me. I've been able to find about 20 Start Up job postings and email their recruiters in about two hours. Your response rates will vary, but I've seen >50% replies to my emails, ie 10 potential interviews from just a few hours of work.","title":"How to find a Data Science job at a Start-Up"},{"location":"how-to-find-a-data-science-job-at-a-startup/#how-to-find-a-data-science-job-at-a-start-up","text":"Getting a job interview for a Data Science role at a Start-Up is challenging: since Start-Up\u2019s are relatively small, their job postings are harder to find, and their processes are less formal than more established companies. In my own experience applying to Start-Ups, I was eventually able to figure out a system that worked very well for me personally. The system itself is pretty simple: use Google to search the HR sites that start-ups use, like jobs.lever.co or greenhouse.io, then find the email address of the recruiter or manager using a free service like Clearbit, and then use a templated email to contact the recruiter directly and ask for an interview. I've outlined the process in more details below, including the exact searches you can use on Google, how to find the recruiter's email, and a template you can try for yourself. Using this system, I was able to solicit multiple Data Science interviews with Start-Up companies in San Francisco that I was genuinely interested in. Best of all, it was much more effective than submitting my resume through online job boards like Indeed or Monster. If you're finding that applying through job boards isn't working, try giving this system a shot. It only takes an hour or so to try it out.","title":"How to find a Data Science job at a Start-Up"},{"location":"how-to-find-a-data-science-job-at-a-startup/#step-1-build-a-list-of-prospective-job-openings","text":"The most effective way I\u2019ve discovered to do this is to use google to search specific sites, using a query like the following. site:jobs.lever.co data scientist San Francisco Note how the use of site:jobs.lever.co filters Google\u2019s results to only the pages on that site, which happens to be all of the job postings on Lever. Note: Lever is a candidate tracking system that\u2019s popular among SF tech companies. Lever is very different than sites like Monster or Indeed because it isn\u2019t a listing site. I\u2019ve also had good results using the site boards.greenhouse.io. You can also use Google\u2019s search tools to only show new listings from the last month, which helps ensure the postings aren\u2019t \u201cstale.\u201d Each result will take you to a job description, and they\u2019ll include the company\u2019s logo with a link to the company\u2019s website. You should be able to find 10\u201320 interesting startups in about an hour or so by using this method and adjusting the titles you use. The postings on Lever allow you to apply directly, but I wouldn\u2019t recommend it. Instead, you should find the email address of the Start Up\u2019s recruiter and contact them directly.","title":"Step 1: Build a list of prospective job openings"},{"location":"how-to-find-a-data-science-job-at-a-startup/#step-2-email-the-start-ups-recruiter-directly","text":"My experience has been that direct emails to a recruiter work much more effectively than submitting to the job posting. It\u2019s also pretty easy to find the recruiter\u2019s email once you know the company\u2019s domain name. First, use LinkedIn to look up the list of that company\u2019s employees. Search for someone that has \u201crecruiter\u201d or a similar title. Their email is almost always the following: firstname@domain.com. Ie if the company\u2019s site was polleverywhere.com and the recruiter was named Tom Waterman, you can just try tom@polleverywhere.com. (If you\u2019re applying to bigger companies, this method is less likely to work). To validate the email address, you can use http://clearbit.com. Their free product allows you to \u201cenrich\u201d an email address, and it also works for checking that the email address you\u2019re using is the right one. (Note: I am not affiliated with Clearbit in any way). But what should you do if the company doesn\u2019t have a recruiter? That\u2019s not uncommon at Start Ups. The next best person to look for is a manager on the team you\u2019re applying to, or even a founder. You might feel nervous about emailing the founders directly, but I promise they won\u2019t find it rude. If anything, they tend to appreciate the initiative. Now that you\u2019ve got the recruiter\u2019s email, you guessed it \u2014 you\u2019re going to have to email them.","title":"Step 2: email the start up\u2019s recruiter directly"},{"location":"how-to-find-a-data-science-job-at-a-startup/#step-3-explicitly-ask-the-recruiter-for-a-phone-call","text":"You might not be sure how to email the recruiter to ask for a phone call. In my experience, the best way is to keep the email extremely brief, attach your resume, and include an explicit request for a phone call. Don't include a wall of text explaining why you\u2019re applying, your professional history, or anything else. Definitely don't address the letter using \u201csir\u201d or \"madam.\" Just state the reason for your email and the action you\u2019re asking them to take. Here\u2019s a template that has worked well for me: Hi [Recruiter first name], I\u2019m reaching out about the Data Science job posting I found on [Company\u2019s website]. Are you free for a short call to discuss whether the role could be a good fit? I\u2019m hoping to learn more about the team\u2019s current projects, and whether my skills and experience could be a good match. About me: Currently a data scientist at Facebook Previously a product analyst for a ~50 person SaaS company Previously an analyst for consulting company doing mainly analytics projects I\u2019ve also attached a resume with more details of my project experience and relevant skills Thanks for your time! Tom","title":"Step 3: explicitly ask the recruiter for a phone call"},{"location":"how-to-find-a-data-science-job-at-a-startup/#step-35-follow-up-if-you-dont-get-a-response","text":"It sucks to have your email ignored, but it will happen. Sometimes the recruiter is ignoring you because the role has been filled, or they don\u2019t think you\u2019re a good candidate, or maybe they were just on vacation. Don\u2019t take it personally \u2014 I\u2019m sure you\u2019ve also not replied to emails, too. Whatever the reason for not getting an email back, sending a follow up email is guaranteed to increase your response rates. I always send a follow up, even if it hurts my pride to revisit the fact that I got ignored in the first place. This system has worked really well for me. I've been able to find about 20 Start Up job postings and email their recruiters in about two hours. Your response rates will vary, but I've seen >50% replies to my emails, ie 10 potential interviews from just a few hours of work.","title":"Step 3.5: follow up if you don\u2019t get a response"},{"location":"installing-airflow-on-ubuntu-18/","text":"Installing Airflow 1.10 on Ubuntu 18 This guide walks through installing Airflow on a Ubuntu 18.04 server. It assumes you can access a web server with a fresh Ubuntu install as the root user. This guide also expects that you're generally familiar with the concepts behind Airflow, like executors, DAGs, and Hooks, but it's not necessary to be an Airflow expert. If you've just started using Airflow, it's helpful to complete the project's quickstart tutorial first, before attempting to deploy a production instance of Airflow. By the time you've finished this guide, you'll have created the following: An Airflow Webserver that's accessible through a web browser. Optionally, we'll show how to enforce HTTPS connections to the Airflow Webserver if you've purchased a custom domain name. An Airflow Scheduler that uses the Celery executor. We'll use a single-node setup, but using the Celery executor will make it easier for you to adapt to a multiple node setup later, and it will also enable you to trigger DAG runs manually through a web browser. Depending on the cloud hosting provider you've chosen for your Ubuntu server, the exact steps for configuring the Webserver will be slightly different, and we can't provide detailed instructions for all the possible vendors. This guide will provide instructions if you're using AWS EC2 instance, but the steps are similar if you're on Digital Ocean, Linode, or another provider. We recommend using a virtual server with at least 2 CPUs and 4G of RAM, but the instructions below should work on smaller instances, too. So, let's get started! Install Airflow dependencies: Postgres, Python3, and Redis For this tutorial we'll use Python 3 and a Postgres database for Airflow's metadata. sudo apt-get update sudo apt-get install -y \\ python3-pip \\ postgresql \\ postgresql-contrib \\ redis \\ nginx The Postgres metadata database is necessary for Airflow to record the state of each task that it runs. The database can also securely store credentials that allow Airflow to connect to other systems, such as Salesforce, S3, or Redshift. Redis is necessary to allow the Airflow Celery Executor to orchestrate its jobs across multiple nodes and to communicate with the Airflow Scheduler. Nginx will be used as a reverse proxy for the Airflow Webserver, and is necessary if you plan to run Airflow on a custom domain, such as airflow.corbettanalytics.com. Configure Postgres When you installed Postgres it also installed commands for creating new Postgres databases and users. Create a new database and user for Airflow, and name them both airflow. sudo -u postgres bash -c \"createdb airflow\" sudo -u postgres bash -c \"createuser airflow --pwprompt\" The createuser command will prompt you for a password for the airflow user. Later, we'll need to store that password in Airflow's configuration file in order for Airflow to function properly. Create a non-root user for Airflow Running Airflow as root is risky, since you'll be executing code from anyone at your company. If they make a mistake and damage the server by accidentally deleting any important files, you'll have to re-provision a new instance. To reduce the chance that happens, we'll create a non-root user to run Airflow. sudo adduser airflow Switch users to the airflow user and cd into their home directory. su airflow cd ~/airflow 3 Install Airflow with pip We're now ready to install Airflow. Note that the package's name is actually apache-airflow and that we're including the optional Postgres and Celery dependencies by appending [postgres,celery] to the package name. We've also included the package email_validator, which is necessary for authenticating with the Airflow webserver. pip3 install --user apache-airflow[postgres,celery,redis] email_validator On Ubuntu18 pip will install the Airflow executable in /home/ubuntu/.local/bin , but by default that directory is not searched when running commands. To fix this, add the .local/bin directory name to your $PATH variable. echo \"export PATH=$PATH:$HOME/.local/bin\" >> ~/.bashrc source ~/.bashrc You should now be able to access the Airflow cli. airflow version # -> 1.10.10 Note that most of Airflow CLI's commands will be broken at this point because the Airflow metadata database isn't configured. To fix this, we'll need edit Airflow's configuration file. Configure Airflow By default, Airflow will look for its configuration in a file located at $HOME/airflow/airflow.cfg . We need to make several changes to that file in order for Airflow to function correctly. Metadata database connection: We need to tell Airflow how to access its metadata database, which we do by setting the sql_alchemy_conn value. That will point to the local Postgres installation we previously configured. Webserver authentication: Airflow supports a few different authentication schemes, but we'll use the default which just requires a username and password. Celery Executor selection: Airflow uses the SequentialExecutor by default, but we'll change that to use the more useful CeleryExecutor. To make these changes, update the following values in the airflow.cfg file. [core] # Connection string to the local Postgres database sql_alchemy_conn = postgresql://airflow:password@localhost:5432/airflow # Class name of the executor executor = CeleryExecutor [webserver] # Run a single gunicorn process for handling requests. workers = 1 # Require password authentication to the webserver authenticate = True rbac = True [celery] broker_url = redis://localhost:6379/0 result_backend = sqla+postgres://airflow:password@localhost:5432/airflow Note that it's important you make these changes in the correct sections - ensure you're replacing the existing values in the configuration, rather than pasting in new lines. If you place the authenticate = True value the [core] section, Airflow won't know to require authentication for the Webserver. Initialize the Airflow metadata database Airflow is now ready to initialize its metadata database. airflow initdb You'll see Airflow log some information about the migrations that it's running on the database. To see the list of tables that Airflow created, use the following command. psql -c '\\dt' At this point the Airflow installation is working correctly, and we're ready to add the first Airflow user. Create an Airflow user The Airflow CLI provides a create_user command to create a user that can log into the airflow Webserver. airflow create_user \\ --username admin \\ --role Admin \\ --email tom@corbettanalytics.com \\ --firstname Tom \\ --lastname Waterman \\ --password {{ SET-PASSWORD-HERE }} You'll now be able to log in with the username and password just configured. Make sure to modify the email and password in the command above to your own values. Start the airflow services For our installation, we'll need to run three Airflow services: the Webserver, the Scheduler, and the Worker. The scheduler is responsible for determining whether a DAG should be executed. The webserver is responsible for creating the DAG visualizations and \"front end\" that you can access from the web browser. If your DAGs are not running, it is likely an issue with the scheduler and not the webserver. The worker is responsible for actually executing the jobs in a DAG. After a working finishes running a DAG's job, it will log the status of the job in the Airflow metadata database. Start the Airflow services now. We'll add the --daemon flag to run the processes as daemons, so they'll continue running even after you log off. airflow webserver --daemon airflow scheduler --daemon airflow worker --daemon You can test that the webserver is running locally. Use curl to test that the Webserver replies with a valid response: you should get a 301 status code redirecting you to /home or /login. curl http://0.0.0.0:8080 At this point, you won't be able to access the Airflow Webserver from your web browser. To access the webserver through your browser, you'll need to have Nginx forward requests to the application. Configure Nginx Switch back to the Ubuntu root user by typing exit. Run the following command to confirm you're no longer using the non-root airflow user. whoami # -> ubuntu Create a new Nginx config file in the /etc/nginx/sites-available directory named airflow . sudo vi /etc/nginx/sites-available/airflow Paste the following into that file. Update the values for server_name with your own server's domain or IP address. server { listen 80; listen [::]:80; # Replace with your own server's URL and/or IP address. Note that # supplying a single value, such as an IP address, is okay. server_name airflow.corbettanalytics.com 34.218.228.207; location / { proxy_pass http://0.0.0.0:8080 } } To enable Nginx to use that site's configuration, link the file into the /etc/nginx/sites-enabled directory. sudo ln -s /etc/nginx/sites-available/airflow /etc/nginx/sites-enabled Additionally, you'll want to update the /etc/nginx/nginx.conf file if you're using a custom domain to host your Airflow server. The follow parameters enables you to use a custom domain, such as airflow.corbettanalytics.com. http { server_names_hash_bucket_size 256; } Restart the Nginx service for the new configuration to take effect. sudo systemctl restart nginx You should now be able to access the Airflow web interface by typing your server's IP address or domain name into your browser's address bar. Note #1 : to access Airflow through the custom domain name, ensure you've already set up an A-record with your DNS provider that points to your server's IP address. If you're using AWS's Route 53, create those records now in the Route 53 console. Note #2 : if you're running the server on AWS, you'll also need to update your server's security group to allow traffic to allow public access to port 80 and port 443. Follow the instructions from AWS's knowledge center to allow access to your server over the public internet. Optional: enforce HTTPS only To enforce HTTPS connections, you'll need to have configured a custom domain that correctly points to your server. Ensure that you can load the Airflow Webserver at your custom domain before proceeding. For example, I was able load Airflow at http://airflow.corbettanalytics.com before configuring HTTPS only connections. Install the Let's Encrypt certbot. sudo apt-get install software-properties-common sudo add-apt-repository universe sudo add-apt-repository ppa:certbot/certbot sudo apt-get update Start the process to automatically configure HTTPS connections to your server. sudo certbot --nginx The Certbot command will prompt you with a series of questions. It should automatically detect that the certificates should be generated for your domain custom domain. For example, Certbot displayed the following details for airflow.corbettanalytics.com. Which names would you like to activate HTTPS for? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: airflow.corbettanalytics.com - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - You should also ensure that you select the option for Certbot to update the Nginx configuration to redirect HTTP requests to HTTPS requests. Certbot will make the necessary changes to the etc/nginx/nginx.conf file and then restart Nginx automatically. If everything has been configured correctly, you should now be able to access your Airflow Webserver over HTTPS.","title":"Installing Airflow 1.10 on Ubuntu 18"},{"location":"installing-airflow-on-ubuntu-18/#installing-airflow-110-on-ubuntu-18","text":"This guide walks through installing Airflow on a Ubuntu 18.04 server. It assumes you can access a web server with a fresh Ubuntu install as the root user. This guide also expects that you're generally familiar with the concepts behind Airflow, like executors, DAGs, and Hooks, but it's not necessary to be an Airflow expert. If you've just started using Airflow, it's helpful to complete the project's quickstart tutorial first, before attempting to deploy a production instance of Airflow. By the time you've finished this guide, you'll have created the following: An Airflow Webserver that's accessible through a web browser. Optionally, we'll show how to enforce HTTPS connections to the Airflow Webserver if you've purchased a custom domain name. An Airflow Scheduler that uses the Celery executor. We'll use a single-node setup, but using the Celery executor will make it easier for you to adapt to a multiple node setup later, and it will also enable you to trigger DAG runs manually through a web browser. Depending on the cloud hosting provider you've chosen for your Ubuntu server, the exact steps for configuring the Webserver will be slightly different, and we can't provide detailed instructions for all the possible vendors. This guide will provide instructions if you're using AWS EC2 instance, but the steps are similar if you're on Digital Ocean, Linode, or another provider. We recommend using a virtual server with at least 2 CPUs and 4G of RAM, but the instructions below should work on smaller instances, too. So, let's get started!","title":"Installing Airflow 1.10 on Ubuntu 18"},{"location":"installing-airflow-on-ubuntu-18/#install-airflow-dependencies-postgres-python3-and-redis","text":"For this tutorial we'll use Python 3 and a Postgres database for Airflow's metadata. sudo apt-get update sudo apt-get install -y \\ python3-pip \\ postgresql \\ postgresql-contrib \\ redis \\ nginx The Postgres metadata database is necessary for Airflow to record the state of each task that it runs. The database can also securely store credentials that allow Airflow to connect to other systems, such as Salesforce, S3, or Redshift. Redis is necessary to allow the Airflow Celery Executor to orchestrate its jobs across multiple nodes and to communicate with the Airflow Scheduler. Nginx will be used as a reverse proxy for the Airflow Webserver, and is necessary if you plan to run Airflow on a custom domain, such as airflow.corbettanalytics.com.","title":"Install Airflow dependencies: Postgres, Python3, and Redis"},{"location":"installing-airflow-on-ubuntu-18/#configure-postgres","text":"When you installed Postgres it also installed commands for creating new Postgres databases and users. Create a new database and user for Airflow, and name them both airflow. sudo -u postgres bash -c \"createdb airflow\" sudo -u postgres bash -c \"createuser airflow --pwprompt\" The createuser command will prompt you for a password for the airflow user. Later, we'll need to store that password in Airflow's configuration file in order for Airflow to function properly.","title":"Configure Postgres"},{"location":"installing-airflow-on-ubuntu-18/#create-a-non-root-user-for-airflow","text":"Running Airflow as root is risky, since you'll be executing code from anyone at your company. If they make a mistake and damage the server by accidentally deleting any important files, you'll have to re-provision a new instance. To reduce the chance that happens, we'll create a non-root user to run Airflow. sudo adduser airflow Switch users to the airflow user and cd into their home directory. su airflow cd ~/airflow","title":"Create a non-root user for Airflow"},{"location":"installing-airflow-on-ubuntu-18/#3-install-airflow-with-pip","text":"We're now ready to install Airflow. Note that the package's name is actually apache-airflow and that we're including the optional Postgres and Celery dependencies by appending [postgres,celery] to the package name. We've also included the package email_validator, which is necessary for authenticating with the Airflow webserver. pip3 install --user apache-airflow[postgres,celery,redis] email_validator On Ubuntu18 pip will install the Airflow executable in /home/ubuntu/.local/bin , but by default that directory is not searched when running commands. To fix this, add the .local/bin directory name to your $PATH variable. echo \"export PATH=$PATH:$HOME/.local/bin\" >> ~/.bashrc source ~/.bashrc You should now be able to access the Airflow cli. airflow version # -> 1.10.10 Note that most of Airflow CLI's commands will be broken at this point because the Airflow metadata database isn't configured. To fix this, we'll need edit Airflow's configuration file.","title":"3 Install Airflow with pip"},{"location":"installing-airflow-on-ubuntu-18/#configure-airflow","text":"By default, Airflow will look for its configuration in a file located at $HOME/airflow/airflow.cfg . We need to make several changes to that file in order for Airflow to function correctly. Metadata database connection: We need to tell Airflow how to access its metadata database, which we do by setting the sql_alchemy_conn value. That will point to the local Postgres installation we previously configured. Webserver authentication: Airflow supports a few different authentication schemes, but we'll use the default which just requires a username and password. Celery Executor selection: Airflow uses the SequentialExecutor by default, but we'll change that to use the more useful CeleryExecutor. To make these changes, update the following values in the airflow.cfg file. [core] # Connection string to the local Postgres database sql_alchemy_conn = postgresql://airflow:password@localhost:5432/airflow # Class name of the executor executor = CeleryExecutor [webserver] # Run a single gunicorn process for handling requests. workers = 1 # Require password authentication to the webserver authenticate = True rbac = True [celery] broker_url = redis://localhost:6379/0 result_backend = sqla+postgres://airflow:password@localhost:5432/airflow Note that it's important you make these changes in the correct sections - ensure you're replacing the existing values in the configuration, rather than pasting in new lines. If you place the authenticate = True value the [core] section, Airflow won't know to require authentication for the Webserver.","title":"Configure Airflow"},{"location":"installing-airflow-on-ubuntu-18/#initialize-the-airflow-metadata-database","text":"Airflow is now ready to initialize its metadata database. airflow initdb You'll see Airflow log some information about the migrations that it's running on the database. To see the list of tables that Airflow created, use the following command. psql -c '\\dt' At this point the Airflow installation is working correctly, and we're ready to add the first Airflow user.","title":"Initialize the Airflow metadata database"},{"location":"installing-airflow-on-ubuntu-18/#create-an-airflow-user","text":"The Airflow CLI provides a create_user command to create a user that can log into the airflow Webserver. airflow create_user \\ --username admin \\ --role Admin \\ --email tom@corbettanalytics.com \\ --firstname Tom \\ --lastname Waterman \\ --password {{ SET-PASSWORD-HERE }} You'll now be able to log in with the username and password just configured. Make sure to modify the email and password in the command above to your own values.","title":"Create an Airflow user"},{"location":"installing-airflow-on-ubuntu-18/#start-the-airflow-services","text":"For our installation, we'll need to run three Airflow services: the Webserver, the Scheduler, and the Worker. The scheduler is responsible for determining whether a DAG should be executed. The webserver is responsible for creating the DAG visualizations and \"front end\" that you can access from the web browser. If your DAGs are not running, it is likely an issue with the scheduler and not the webserver. The worker is responsible for actually executing the jobs in a DAG. After a working finishes running a DAG's job, it will log the status of the job in the Airflow metadata database. Start the Airflow services now. We'll add the --daemon flag to run the processes as daemons, so they'll continue running even after you log off. airflow webserver --daemon airflow scheduler --daemon airflow worker --daemon You can test that the webserver is running locally. Use curl to test that the Webserver replies with a valid response: you should get a 301 status code redirecting you to /home or /login. curl http://0.0.0.0:8080 At this point, you won't be able to access the Airflow Webserver from your web browser. To access the webserver through your browser, you'll need to have Nginx forward requests to the application.","title":"Start the airflow services"},{"location":"installing-airflow-on-ubuntu-18/#configure-nginx","text":"Switch back to the Ubuntu root user by typing exit. Run the following command to confirm you're no longer using the non-root airflow user. whoami # -> ubuntu Create a new Nginx config file in the /etc/nginx/sites-available directory named airflow . sudo vi /etc/nginx/sites-available/airflow Paste the following into that file. Update the values for server_name with your own server's domain or IP address. server { listen 80; listen [::]:80; # Replace with your own server's URL and/or IP address. Note that # supplying a single value, such as an IP address, is okay. server_name airflow.corbettanalytics.com 34.218.228.207; location / { proxy_pass http://0.0.0.0:8080 } } To enable Nginx to use that site's configuration, link the file into the /etc/nginx/sites-enabled directory. sudo ln -s /etc/nginx/sites-available/airflow /etc/nginx/sites-enabled Additionally, you'll want to update the /etc/nginx/nginx.conf file if you're using a custom domain to host your Airflow server. The follow parameters enables you to use a custom domain, such as airflow.corbettanalytics.com. http { server_names_hash_bucket_size 256; } Restart the Nginx service for the new configuration to take effect. sudo systemctl restart nginx You should now be able to access the Airflow web interface by typing your server's IP address or domain name into your browser's address bar. Note #1 : to access Airflow through the custom domain name, ensure you've already set up an A-record with your DNS provider that points to your server's IP address. If you're using AWS's Route 53, create those records now in the Route 53 console. Note #2 : if you're running the server on AWS, you'll also need to update your server's security group to allow traffic to allow public access to port 80 and port 443. Follow the instructions from AWS's knowledge center to allow access to your server over the public internet.","title":"Configure Nginx"},{"location":"installing-airflow-on-ubuntu-18/#optional-enforce-https-only","text":"To enforce HTTPS connections, you'll need to have configured a custom domain that correctly points to your server. Ensure that you can load the Airflow Webserver at your custom domain before proceeding. For example, I was able load Airflow at http://airflow.corbettanalytics.com before configuring HTTPS only connections. Install the Let's Encrypt certbot. sudo apt-get install software-properties-common sudo add-apt-repository universe sudo add-apt-repository ppa:certbot/certbot sudo apt-get update Start the process to automatically configure HTTPS connections to your server. sudo certbot --nginx The Certbot command will prompt you with a series of questions. It should automatically detect that the certificates should be generated for your domain custom domain. For example, Certbot displayed the following details for airflow.corbettanalytics.com. Which names would you like to activate HTTPS for? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: airflow.corbettanalytics.com - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - You should also ensure that you select the option for Certbot to update the Nginx configuration to redirect HTTP requests to HTTPS requests. Certbot will make the necessary changes to the etc/nginx/nginx.conf file and then restart Nginx automatically. If everything has been configured correctly, you should now be able to access your Airflow Webserver over HTTPS.","title":"Optional: enforce HTTPS only"},{"location":"job_descriptions_for_analytics_engineers/","text":"Examples of what not to do: Need to have an understanding of GCP data The confusion here is believing that the data host is Comfortable working with open-source technologies. I'm not sure the thinking behind this one. Maybe they're just trying to get the word \"open-source\" into the job description, since they believe developers like hearing \"open source.\" data visualization tools like D3.js","title":"Job descriptions for analytics engineers"},{"location":"job_descriptions_for_analytics_engineers/#examples-of-what-not-to-do","text":"Need to have an understanding of GCP data The confusion here is believing that the data host is Comfortable working with open-source technologies. I'm not sure the thinking behind this one. Maybe they're just trying to get the word \"open-source\" into the job description, since they believe developers like hearing \"open source.\" data visualization tools like D3.js","title":"Examples of what not to do:"},{"location":"not-liking-fiction-is-okay/","text":"It's okay not to like fiction When I was in college and high-school there was a huge emphasis on reading fiction: James Joyce, Dante, Zora Neal Hurston, Shakespeare, Aravand Adiga, Mark Twain, Aldous Huxely, Ralph Ellison, Gabriel Garcia Marquez, and on and on. If we were ever assigned any non-fiction, they were over-structured text-books about history or ancient Greek philosophy. I could never get into those stories at all. I'm sure part of that was a lack of appreciation for the stories, since it's hard as an 18 year old to grok the point the author is trying to make, especially when that author would use sentences that were 300 words long. But some of the lack of appreciation was also an intentional part of the curriculum. The stories we were assigned to read were almost always stories that were far outside of our normal experience.","title":"It's okay not to like fiction"},{"location":"not-liking-fiction-is-okay/#its-okay-not-to-like-fiction","text":"When I was in college and high-school there was a huge emphasis on reading fiction: James Joyce, Dante, Zora Neal Hurston, Shakespeare, Aravand Adiga, Mark Twain, Aldous Huxely, Ralph Ellison, Gabriel Garcia Marquez, and on and on. If we were ever assigned any non-fiction, they were over-structured text-books about history or ancient Greek philosophy. I could never get into those stories at all. I'm sure part of that was a lack of appreciation for the stories, since it's hard as an 18 year old to grok the point the author is trying to make, especially when that author would use sentences that were 300 words long. But some of the lack of appreciation was also an intentional part of the curriculum. The stories we were assigned to read were almost always stories that were far outside of our normal experience.","title":"It's okay not to like fiction"},{"location":"presenting-analytics-researching-vs-lawyering/","text":"Presenting Analytics: Researching vs. Lawyering I've noticed there are two \"rhetorical strategies\" that almost all business-related presentations follow. The first strategy, which I've called researching, is about explaining what's happened and why it happened that way: Revenue growth was lagging this quarter as we had fewer repeat customers come back through our re-marketing channels. The second strategy, which I've called lawyering, is about getting the audience to respond to the presentation in a specific way. We need to scale up our re-marketing campaigns this quarter in order to hit our end of year revenue goals. Most people tend to prefer one strategy over the other, and they're reluctant to ever use both. For example, some people find the directness of the lawyering strategy manipulative, and they view the people that use that kind of rhetoric as ruthless mercenaries willing to say whatever it takes to earn a buck. They might think something like - \"Of course you want us to spend more on re-marketing, you're the re-marketing person! \" Similarly, some people find the researching strategy tedious to follow, and they'll get frustrated listening to the presentation because it feels unproductive. They might think something like - \"I know our revenue is down, but my question is what are we supposed to do about it? We're losing time here! \" Most people (especially non-technical people) actually tend to prefer the lawyering style. Of course, most people wouldn't ask those questions out loud because they're a little bit rude. But if you've ever had someone give you feedback about your \"presentation style,\" there's a very good chance what they're really asking for is for you to be more \"lawyering.\" Everyone gets this feedback from someone eventually, and I got it early on my career from someone who appreciated that it was just a rhetorical strategy. He called it \"direct presentation style,\" but the idea was the same. Most analytics presentations tend to follow the researching strategy, because they're written by researchers. The strategies shouldn't be viewed as an \"either or.\" You should pick whichever makes the most sense for your context. Pitch deck template from YC follows the researching strategy, and the closest it gets to lawyering is in the last slide where they define the amount of money needed. The researching strategy works well in this context because I need another example to show 'lawyering' at its most effective. Fitting the facts to the story, vs fitting the store to the facts.","title":"Presenting Analytics: Researching vs. Lawyering"},{"location":"presenting-analytics-researching-vs-lawyering/#presenting-analytics-researching-vs-lawyering","text":"I've noticed there are two \"rhetorical strategies\" that almost all business-related presentations follow. The first strategy, which I've called researching, is about explaining what's happened and why it happened that way: Revenue growth was lagging this quarter as we had fewer repeat customers come back through our re-marketing channels. The second strategy, which I've called lawyering, is about getting the audience to respond to the presentation in a specific way. We need to scale up our re-marketing campaigns this quarter in order to hit our end of year revenue goals. Most people tend to prefer one strategy over the other, and they're reluctant to ever use both. For example, some people find the directness of the lawyering strategy manipulative, and they view the people that use that kind of rhetoric as ruthless mercenaries willing to say whatever it takes to earn a buck. They might think something like - \"Of course you want us to spend more on re-marketing, you're the re-marketing person! \" Similarly, some people find the researching strategy tedious to follow, and they'll get frustrated listening to the presentation because it feels unproductive. They might think something like - \"I know our revenue is down, but my question is what are we supposed to do about it? We're losing time here! \" Most people (especially non-technical people) actually tend to prefer the lawyering style. Of course, most people wouldn't ask those questions out loud because they're a little bit rude. But if you've ever had someone give you feedback about your \"presentation style,\" there's a very good chance what they're really asking for is for you to be more \"lawyering.\" Everyone gets this feedback from someone eventually, and I got it early on my career from someone who appreciated that it was just a rhetorical strategy. He called it \"direct presentation style,\" but the idea was the same. Most analytics presentations tend to follow the researching strategy, because they're written by researchers. The strategies shouldn't be viewed as an \"either or.\" You should pick whichever makes the most sense for your context. Pitch deck template from YC follows the researching strategy, and the closest it gets to lawyering is in the last slide where they define the amount of money needed. The researching strategy works well in this context because I need another example to show 'lawyering' at its most effective. Fitting the facts to the story, vs fitting the store to the facts.","title":"Presenting Analytics: Researching vs. Lawyering"},{"location":"productivity/","text":"Productivity Either you can prioritize responsiveness or deep focus. If you prioritize deep focus, you need to have some level of autonomy. Being completely reliant on someone else for a key piece of information that's blocking your work will disrupt your focus. Complex projects are harder to achieve deep focus: you need input from others. Blocking off time for being unresponive . Some people think managers / execs need to prioritize responsiveness, but that's definitely not the case. Priotizing focus also also means you're more \"present\" when you are with other people. That helps make them feel important and increases the quality of your contributions. Handling stress is another distraction. If you're too stretched out on many projects, you won't be able to focus. Need to find a way to feel good aobut what you're prioritizing. Some days are much more productive than others: you slept well, you're excited. Playing mental tricks with yourself. Getting started is the hardest part. \"Put on my shoes.\" \"Walk out the door.\" \"Open my text editor.\" Public and private \"tasks queue.\" Tooling matters a lot. It's in vogue for managers to say \"software can't fix this problem!\" I think it's a way for them to strut and flex their \"EQ\" muscles, but there's something ironic about that. Software is never a complete solution.","title":"Productivity"},{"location":"productivity/#productivity","text":"Either you can prioritize responsiveness or deep focus. If you prioritize deep focus, you need to have some level of autonomy. Being completely reliant on someone else for a key piece of information that's blocking your work will disrupt your focus. Complex projects are harder to achieve deep focus: you need input from others. Blocking off time for being unresponive . Some people think managers / execs need to prioritize responsiveness, but that's definitely not the case. Priotizing focus also also means you're more \"present\" when you are with other people. That helps make them feel important and increases the quality of your contributions. Handling stress is another distraction. If you're too stretched out on many projects, you won't be able to focus. Need to find a way to feel good aobut what you're prioritizing. Some days are much more productive than others: you slept well, you're excited. Playing mental tricks with yourself. Getting started is the hardest part. \"Put on my shoes.\" \"Walk out the door.\" \"Open my text editor.\" Public and private \"tasks queue.\" Tooling matters a lot. It's in vogue for managers to say \"software can't fix this problem!\" I think it's a way for them to strut and flex their \"EQ\" muscles, but there's something ironic about that. Software is never a complete solution.","title":"Productivity"},{"location":"schema-on-read-for-analytics-please/","text":"Schema on read for analytics, please Just load your JSON and stop rejecting \"schema\" issues Defining a schema \"on read\" gets a bad rep because of applications that use \"schemaless\" databases like MongoDB. Invariably those schemaless designs come back to bite you: dirty data gets in the database, and now your users have bugs that are difficult to find and are hard to fix. For most types of applications, validating data before it enters the database is critical, and a schema enforced by the database itself is the method that has worked since the 1970s. I think you'd be crazy to not enforce a schema for your web app's database. But in analytics, data is different. The data that your logging systems send is only ever sent once and it should never be mutated: a user signs up and you log a signup event, or a user views a page and you log a pageview event. Should you reject that data if they have an encoding error? Of course not, throwing away those events completely invalidates your data. But that data still feels wrong, so databases try to enforce a schema and \"quarantine\" data that doesn't match what they expect. Maybe that quarantine is just an s3 bucket, and the designers of that schema feel happy they've created such a \"safe\" system with only \"clean\" data. But would it really be so bad to expose consumers to that data, and ask them to clean it themselves? Does that data really need to be cleaned before it enters the analytics database? Can it really only be cleaned by data engineers? Consider what happens when the data gets quarantined instead of loading it. A data engineer now needs to review the rejected data each day. They manually apply some transformations (hopefully not to the raw data itself, because that's causing data loss), and eventually they create a version that's fixed up and \"clean.\" They then manually load it into the analytics database once they're done, and all their effort feels very productive: no one was exposed to that terrible, dirty data! In reality, all they're doing is spending huge amounts of time managing a duplicate system from the analytics database, completing work that is far more effectively completed by the downstream consumers themselves, and delaying those consumers from their projects. Of course, it helps that they've created a system that's \"necessary\" and requires their personal management each day. meanwhile, take the plunge and allow a schemaless ingestion, just taking the dirty data as is, which is almost always just a simple JSON object. Anyone using that data has to do the work that only they can do: validate it's clean for their purposes. With a data modelling framework like dbt, it's also easy for them to share that cleaning back to the analytics warehouse so other users can use it as well. They don't need to ask a data engineer where their data went, and how long until it's available. Of course, schemaless loading doesn't mean the data needs to stays schemaless. Once the data has been loaded, any issues with the schema can be handled by the analysts consuming it. I promise you, it's going to be okay. Load your JSON. Let your analysts play with it. If you really can't trust the analysts to handle the difference between Account_id and account_id , trying to fix that problem by automating your schema validation isn't going to work.","title":"Schema on read for analytics, please"},{"location":"schema-on-read-for-analytics-please/#schema-on-read-for-analytics-please","text":"Just load your JSON and stop rejecting \"schema\" issues Defining a schema \"on read\" gets a bad rep because of applications that use \"schemaless\" databases like MongoDB. Invariably those schemaless designs come back to bite you: dirty data gets in the database, and now your users have bugs that are difficult to find and are hard to fix. For most types of applications, validating data before it enters the database is critical, and a schema enforced by the database itself is the method that has worked since the 1970s. I think you'd be crazy to not enforce a schema for your web app's database. But in analytics, data is different. The data that your logging systems send is only ever sent once and it should never be mutated: a user signs up and you log a signup event, or a user views a page and you log a pageview event. Should you reject that data if they have an encoding error? Of course not, throwing away those events completely invalidates your data. But that data still feels wrong, so databases try to enforce a schema and \"quarantine\" data that doesn't match what they expect. Maybe that quarantine is just an s3 bucket, and the designers of that schema feel happy they've created such a \"safe\" system with only \"clean\" data. But would it really be so bad to expose consumers to that data, and ask them to clean it themselves? Does that data really need to be cleaned before it enters the analytics database? Can it really only be cleaned by data engineers? Consider what happens when the data gets quarantined instead of loading it. A data engineer now needs to review the rejected data each day. They manually apply some transformations (hopefully not to the raw data itself, because that's causing data loss), and eventually they create a version that's fixed up and \"clean.\" They then manually load it into the analytics database once they're done, and all their effort feels very productive: no one was exposed to that terrible, dirty data! In reality, all they're doing is spending huge amounts of time managing a duplicate system from the analytics database, completing work that is far more effectively completed by the downstream consumers themselves, and delaying those consumers from their projects. Of course, it helps that they've created a system that's \"necessary\" and requires their personal management each day. meanwhile, take the plunge and allow a schemaless ingestion, just taking the dirty data as is, which is almost always just a simple JSON object. Anyone using that data has to do the work that only they can do: validate it's clean for their purposes. With a data modelling framework like dbt, it's also easy for them to share that cleaning back to the analytics warehouse so other users can use it as well. They don't need to ask a data engineer where their data went, and how long until it's available. Of course, schemaless loading doesn't mean the data needs to stays schemaless. Once the data has been loaded, any issues with the schema can be handled by the analysts consuming it. I promise you, it's going to be okay. Load your JSON. Let your analysts play with it. If you really can't trust the analysts to handle the difference between Account_id and account_id , trying to fix that problem by automating your schema validation isn't going to work.","title":"Schema on read for analytics, please"},{"location":"thoughts-about-alteryx/","text":"What's up with Alteryx? Recently I chatted with a former coworker who was going to join Alteryx as a manager in their analytics group. We'd both used Alteryx at our first job, so I knew that it was really a competitor to the kind of analytics engineering function that's getting more popular in the Bay Area. The biggest reason it's a competitor is that Alteryx strongly encourages you to extract your data out of a data warehouse or data lake, and onto the same computer that you've installed Alteryx. That also means a windows box by the way, since Alteryx doesn't support Mac or Linux at the time I'm writing this. Analytics engineering on the other hand is all about taking advantage of the capabilities of modern data warehouses like Snowflake, and maybe soon the lakehouses like Databricks. So investors have described Alteryx's product recently by saying that it's \"not cloud.\" There's no website you can log into and try Alteryx from your browser. You can't just add more nodes to your Alteryx cluster when you need more speed. But Alteryx does occupy an amazingly lucrative segment in the analytics-tooling space: no code environments. The market for those kinds of products, including low-code app development environments, must be in the 10's of billions of dollars a year, and Alteryx itself took in about $500M in 2020. What's going to happen to a product like that? On the one hand, Alteryx doesn't scale well for customers that need cloud-scale to handle their data, and it places a lot of burden on customers to upgrade their installation, manage their VMS, and so on. But the market they're serving has to be growing like crazy: companies must be hiring far more new analysts every year than they're hiring developers, and the vast majority of those analysts either can't or won't learn SQL for any reason. There's definitely a spot for a company that can build a no-code UI to help analysts write their queries and send them to a database. Alteryx might be the company to do that, and getting it working \"cloud first\" is the easy part. Getting their drag-and-drop workflow to compile correctly to SQL every time is the part that could take an army of engineers. My guess is they'll hit $1B in the next 5 years, and they only need a 15% CAGR to do it. Their sales multiple of 11x is relatively low right now compared to other software products too - that's similar to the megacaps like Microsoft. On paper they're cheap. I'm also optimistic they'll be able to build out their product in a way that better leverages the benefits of the cloud. They might be a competitor in a way to analytics engineering, but there's definitely room for two approaches, and they might be a good pickup while the market has them down.","title":"What's up with Alteryx?"},{"location":"thoughts-about-alteryx/#whats-up-with-alteryx","text":"Recently I chatted with a former coworker who was going to join Alteryx as a manager in their analytics group. We'd both used Alteryx at our first job, so I knew that it was really a competitor to the kind of analytics engineering function that's getting more popular in the Bay Area. The biggest reason it's a competitor is that Alteryx strongly encourages you to extract your data out of a data warehouse or data lake, and onto the same computer that you've installed Alteryx. That also means a windows box by the way, since Alteryx doesn't support Mac or Linux at the time I'm writing this. Analytics engineering on the other hand is all about taking advantage of the capabilities of modern data warehouses like Snowflake, and maybe soon the lakehouses like Databricks. So investors have described Alteryx's product recently by saying that it's \"not cloud.\" There's no website you can log into and try Alteryx from your browser. You can't just add more nodes to your Alteryx cluster when you need more speed. But Alteryx does occupy an amazingly lucrative segment in the analytics-tooling space: no code environments. The market for those kinds of products, including low-code app development environments, must be in the 10's of billions of dollars a year, and Alteryx itself took in about $500M in 2020. What's going to happen to a product like that? On the one hand, Alteryx doesn't scale well for customers that need cloud-scale to handle their data, and it places a lot of burden on customers to upgrade their installation, manage their VMS, and so on. But the market they're serving has to be growing like crazy: companies must be hiring far more new analysts every year than they're hiring developers, and the vast majority of those analysts either can't or won't learn SQL for any reason. There's definitely a spot for a company that can build a no-code UI to help analysts write their queries and send them to a database. Alteryx might be the company to do that, and getting it working \"cloud first\" is the easy part. Getting their drag-and-drop workflow to compile correctly to SQL every time is the part that could take an army of engineers. My guess is they'll hit $1B in the next 5 years, and they only need a 15% CAGR to do it. Their sales multiple of 11x is relatively low right now compared to other software products too - that's similar to the megacaps like Microsoft. On paper they're cheap. I'm also optimistic they'll be able to build out their product in a way that better leverages the benefits of the cloud. They might be a competitor in a way to analytics engineering, but there's definitely room for two approaches, and they might be a good pickup while the market has them down.","title":"What's up with Alteryx?"},{"location":"thoughts-about-online-privacy/","text":"Thoughts about privacy Privacy isn't an absolute standard - there's no algorithm that can decide whether something is \"private\" or not. People feel differently about privacy, and overall Americans and Europeans are much more culturally concered about privacy than other parts of the world, that have different cultural traditions. Also important to understand that there are few legal gaurantees about privacy that absolutely limit the United States government. The government can procure a search warrant and access almost anything they want: recording your phone calls, watching your internet traffic, and recording your location. First party tracking How safeway does privacy today, which no one is concerned by: CCTV footage, \"club cards\" for tracking purchases, also tracking across your credit card number. Many people are actually very excited by the features enabled from first-party tracking - think content recommendations from Netflix, your instagram and tiktok feeds. How amazon does privacy today: every pageview, click, whether or not you have an account. But consumers don't get anything in exchange explicitly, so they feel it's not an agreement, but an invasion. Third party tracking Third party cookies, the \"facebook pixel\" Netflix movie on privacy that sent its cookies to Facebook Currently being limited as much as possible, ie Iphone tracking also being limited. But it's not really possible to limit this effectively from a technical perspective, and there just isn't much motivation for legal restrictions either. Biometric tracking You might feel that your DNA is private, but you share your DNA with so many relatives that realistically it's already not private, and its possible to identify you through \"genetic geneoalogy\" today. Commercial uses for nefarious reasons, such as healthcare coverage or insurance are pretty unlikely. The governments already limit the information those companies can use in their pricing, such as your race, so it's quite likely they'll just limit use of DNA as well. The government itself can also be restrained by laws in how it uses DNA, and it's hard to see a use-case for them that's nefarious. The government's ability to offer more efficient, safer services based on biometric identification could actually be a huge benefit for citizens. How will this affect things like immigration?","title":"Thoughts about privacy"},{"location":"thoughts-about-online-privacy/#thoughts-about-privacy","text":"Privacy isn't an absolute standard - there's no algorithm that can decide whether something is \"private\" or not. People feel differently about privacy, and overall Americans and Europeans are much more culturally concered about privacy than other parts of the world, that have different cultural traditions. Also important to understand that there are few legal gaurantees about privacy that absolutely limit the United States government. The government can procure a search warrant and access almost anything they want: recording your phone calls, watching your internet traffic, and recording your location.","title":"Thoughts about privacy"},{"location":"thoughts-about-online-privacy/#first-party-tracking","text":"How safeway does privacy today, which no one is concerned by: CCTV footage, \"club cards\" for tracking purchases, also tracking across your credit card number. Many people are actually very excited by the features enabled from first-party tracking - think content recommendations from Netflix, your instagram and tiktok feeds. How amazon does privacy today: every pageview, click, whether or not you have an account. But consumers don't get anything in exchange explicitly, so they feel it's not an agreement, but an invasion.","title":"First party tracking"},{"location":"thoughts-about-online-privacy/#third-party-tracking","text":"Third party cookies, the \"facebook pixel\" Netflix movie on privacy that sent its cookies to Facebook Currently being limited as much as possible, ie Iphone tracking also being limited. But it's not really possible to limit this effectively from a technical perspective, and there just isn't much motivation for legal restrictions either.","title":"Third party tracking"},{"location":"thoughts-about-online-privacy/#biometric-tracking","text":"You might feel that your DNA is private, but you share your DNA with so many relatives that realistically it's already not private, and its possible to identify you through \"genetic geneoalogy\" today. Commercial uses for nefarious reasons, such as healthcare coverage or insurance are pretty unlikely. The governments already limit the information those companies can use in their pricing, such as your race, so it's quite likely they'll just limit use of DNA as well. The government itself can also be restrained by laws in how it uses DNA, and it's hard to see a use-case for them that's nefarious. The government's ability to offer more efficient, safer services based on biometric identification could actually be a huge benefit for citizens. How will this affect things like immigration?","title":"Biometric tracking"},{"location":"web-scraping-with-github-actions/","text":"Web Scraping with Github Actions The tools available for building web scrapers have improved significantly since my very first webscraping project back in 2015. Maybe the most interesting new tool for building web scrapers is Github Actions , a service for scheduling any code you want to run directly on Github. Using Github actions, you don't have to set up a server to run your scraper every night. Instead, you can configure Github Actions to run your scripts on a schedule, and it automatically starts and stops a server for you. For a web scraping project, you might have Github actions run a script to scrape the data from a website, and then save save the scraped data to your repository. Github actions is also free to use for public repositories, so building a web scraper won't cost any money. That said, there are certain limits: you can run your scripts for 3,000 minutes per month, no individual file in your repo can be larger than 100MB, and the total repository can not be larger than 10GB. But that's actually more than enough time and space for even large web scraping projects: with 10GB you can store millions of whole web pages, and billions of parsed web pages. Github actions is also quite easy to set up, since it uses a yaml file for configuring how to run your script. Here's a simple example: name: Scrape # The name of your Workflow on: schedule: # Tells Github to run this Workflow every night - cron: \"23 8 * * *\" jobs: scrape-latest: runs-on: ubuntu-latest steps: - name: Checkout repo uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2.0.0 with: python-version: '3.7' # Tells Github to run the file `scraper.py` from your repository - name: Run Scraper run: python scraper.py scrape The script that actually downloads the data is just the python file scraper.py - you can see that the script is being ran in the last step of the workflow above. Once you've scraped the data you want, you can then publish that data using Github releases , making its data easily available for others. I wanted to try out this strategy myself, so I built a US Box Office Revenues tracker based on Github actions. You can download the latest polished dataset yourself. import pandas as pd url = 'https://github.com/tjwaterman99/boxofficemojo-scraper/releases/latest/download/revenues_per_day.csv.gz' df = pd.read_csv(url, parse_dates=['date'], index_col='id') df.head() The scraper that produces that dataset follows the strategy pretty much exactly: each day Github Actions runs a script that downloads the latest revenue data from boxofficemojo.com and saves it to the repository. Another script then parses the scraped data and publishes a polished dataset that's easy to download from a single url on the project's releases page . Overall I've been very impressed with Github's features - Github actions for running a CI/CD service, managed releases, and now even Codespaces , an on-demand development environment. With other companies like Heroku, Netlify, and Snowflake, it's never been faster or easier to build software. It's genuinely amazing how far developer tools have come since 2015.","title":"Web Scraping with Github Actions"},{"location":"web-scraping-with-github-actions/#web-scraping-with-github-actions","text":"The tools available for building web scrapers have improved significantly since my very first webscraping project back in 2015. Maybe the most interesting new tool for building web scrapers is Github Actions , a service for scheduling any code you want to run directly on Github. Using Github actions, you don't have to set up a server to run your scraper every night. Instead, you can configure Github Actions to run your scripts on a schedule, and it automatically starts and stops a server for you. For a web scraping project, you might have Github actions run a script to scrape the data from a website, and then save save the scraped data to your repository. Github actions is also free to use for public repositories, so building a web scraper won't cost any money. That said, there are certain limits: you can run your scripts for 3,000 minutes per month, no individual file in your repo can be larger than 100MB, and the total repository can not be larger than 10GB. But that's actually more than enough time and space for even large web scraping projects: with 10GB you can store millions of whole web pages, and billions of parsed web pages. Github actions is also quite easy to set up, since it uses a yaml file for configuring how to run your script. Here's a simple example: name: Scrape # The name of your Workflow on: schedule: # Tells Github to run this Workflow every night - cron: \"23 8 * * *\" jobs: scrape-latest: runs-on: ubuntu-latest steps: - name: Checkout repo uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2.0.0 with: python-version: '3.7' # Tells Github to run the file `scraper.py` from your repository - name: Run Scraper run: python scraper.py scrape The script that actually downloads the data is just the python file scraper.py - you can see that the script is being ran in the last step of the workflow above. Once you've scraped the data you want, you can then publish that data using Github releases , making its data easily available for others. I wanted to try out this strategy myself, so I built a US Box Office Revenues tracker based on Github actions. You can download the latest polished dataset yourself. import pandas as pd url = 'https://github.com/tjwaterman99/boxofficemojo-scraper/releases/latest/download/revenues_per_day.csv.gz' df = pd.read_csv(url, parse_dates=['date'], index_col='id') df.head() The scraper that produces that dataset follows the strategy pretty much exactly: each day Github Actions runs a script that downloads the latest revenue data from boxofficemojo.com and saves it to the repository. Another script then parses the scraped data and publishes a polished dataset that's easy to download from a single url on the project's releases page . Overall I've been very impressed with Github's features - Github actions for running a CI/CD service, managed releases, and now even Codespaces , an on-demand development environment. With other companies like Heroku, Netlify, and Snowflake, it's never been faster or easier to build software. It's genuinely amazing how far developer tools have come since 2015.","title":"Web Scraping with Github Actions"},{"location":"what-is-an-analytics-engineer/","text":"What is an analytics engineer? Today, analytics engineering is defined by using Data Build Tool (DBT) for building and managing the datasets in an organization's data warehouse. As one of the founders has said (and I think it's fair): their product \"has come to be synonymous with the practice of analytics engineering, defining an entire industry.\" In fact the creators of DBT were the first to start using the title as far as I can tell, probably to try and distinguish their work from that of data engineers. I've heard them describe analytics engineering as the \"Transform\" part of \"Extract, Load, Transform,\" and I think that's exactly right: by using DBT, anyone can transform the raw data in their warehouses into much more useful forms . But defining the role of analytics engineering as synonymous with the tools that it requires feels like it's underselling the job. It's like defining software engineers as people that write Java, or defining consultants as people that make PowerPoint slides. As someone that's about to start an analytics engineering role in two weeks, I wanted to be able to define the role more by what I enabled the company to do, and less by the tools that I used to make that happen. What was I going to create at the company, beyond setting up a DBT repo and building some datasets in their warehouse? Right now, I think investments at a company in analytics engineering is about improving these four capabilities: Speed: Trust: New Data Products: Data Literacy: Improving Speed enabling analysts to contribute Increasing Trust Waiting to make decisions before the data is there Being more researchers than lawyers in how the data is used. What undermines that is usually speed, and/or being cynical. Building New Data Products Creating Data Literacy","title":"What is an analytics engineer?"},{"location":"what-is-an-analytics-engineer/#what-is-an-analytics-engineer","text":"Today, analytics engineering is defined by using Data Build Tool (DBT) for building and managing the datasets in an organization's data warehouse. As one of the founders has said (and I think it's fair): their product \"has come to be synonymous with the practice of analytics engineering, defining an entire industry.\" In fact the creators of DBT were the first to start using the title as far as I can tell, probably to try and distinguish their work from that of data engineers. I've heard them describe analytics engineering as the \"Transform\" part of \"Extract, Load, Transform,\" and I think that's exactly right: by using DBT, anyone can transform the raw data in their warehouses into much more useful forms . But defining the role of analytics engineering as synonymous with the tools that it requires feels like it's underselling the job. It's like defining software engineers as people that write Java, or defining consultants as people that make PowerPoint slides. As someone that's about to start an analytics engineering role in two weeks, I wanted to be able to define the role more by what I enabled the company to do, and less by the tools that I used to make that happen. What was I going to create at the company, beyond setting up a DBT repo and building some datasets in their warehouse? Right now, I think investments at a company in analytics engineering is about improving these four capabilities: Speed: Trust: New Data Products: Data Literacy:","title":"What is an analytics engineer?"},{"location":"what-is-an-analytics-engineer/#improving-speed","text":"enabling analysts to contribute","title":"Improving Speed"},{"location":"what-is-an-analytics-engineer/#increasing-trust","text":"Waiting to make decisions before the data is there Being more researchers than lawyers in how the data is used. What undermines that is usually speed, and/or being cynical.","title":"Increasing Trust"},{"location":"what-is-an-analytics-engineer/#building-new-data-products","text":"","title":"Building New Data Products"},{"location":"what-is-an-analytics-engineer/#creating-data-literacy","text":"","title":"Creating Data Literacy"},{"location":"what-problems-should-be-hard/","text":"What problems should be hard? I remember at my job a few years ago there was a programming book lying on the coffee table called \" practical object oriented design in Ruby ,\" by Sandi Metz. The author had an amazing sentence that summarized the problem the book was solving for: You know how to write the code, but not where to put it. That's probably a surprising problem for most people to hear. Just getting the code to do the thing you hope it does can often be incredibly challenging, unless you've written code to do that thing many times before. Most of us just feel relieved when the thing finally does what we need it to do, and then we sign off for the day and happily look forward to doing something else tomorrow. But if you've ever worked with someone else's code, where your goal isn't to write the thing and get it working, but to change it slightly without breaking it , then you've undoubtably experienced the problem that Sandi is writing about. It's usually far more difficult to change something than it is to get it working the first time, especially when you're working in a team. So \"putting the code in the right place\" is the difference between productive systems where teams can collaborate together, and impossible to understand systems that are extremely challenging to build on. But how do you know your design is actually good? What do you look for to predict whether a system will be easy to adapt? It's hard to actually know the answer unless you've worked with different systems, and you've seen how the exact same problem can be much harder on one system than the other. Unfortunately, the ability to evaluate design is much more about tacit knowledge, rather than explicit principles. Still, I want to say there's rule of thumb that anyone can apply, and that is \"easy problems should be easy.\" You can usually get a good sense of whether a system is productive by trying to do the simplest possible addition (and remember additions are easier than changes). Was it hard? Then the system probably has a design problem. That's the difference between good code bases and code bases that are a struggle to work with. I've noticed that the good code bases will often have a tutorial that proudly demos just how easy it is to get started, because the designers care about it and want to make it as easy as possible for others to collaborate. Of course, a well-designed system has its own trade-offs as well. You probably don't need a good design unless multiple people are working on the project, since it's enough to \"keep it all in your head.\" You might also see systems that are \"over-designed,\" in the sense that they're trying to solve problems that might exist in the future, but don't exist today. But it's worth revisiting the question about design on every change that gets made to the system you're building. Does this change make it harder to solve the easy problems? If the answer is yes, then the change had better be doing something remarkable.","title":"What problems should be hard?"},{"location":"what-problems-should-be-hard/#what-problems-should-be-hard","text":"I remember at my job a few years ago there was a programming book lying on the coffee table called \" practical object oriented design in Ruby ,\" by Sandi Metz. The author had an amazing sentence that summarized the problem the book was solving for: You know how to write the code, but not where to put it. That's probably a surprising problem for most people to hear. Just getting the code to do the thing you hope it does can often be incredibly challenging, unless you've written code to do that thing many times before. Most of us just feel relieved when the thing finally does what we need it to do, and then we sign off for the day and happily look forward to doing something else tomorrow. But if you've ever worked with someone else's code, where your goal isn't to write the thing and get it working, but to change it slightly without breaking it , then you've undoubtably experienced the problem that Sandi is writing about. It's usually far more difficult to change something than it is to get it working the first time, especially when you're working in a team. So \"putting the code in the right place\" is the difference between productive systems where teams can collaborate together, and impossible to understand systems that are extremely challenging to build on. But how do you know your design is actually good? What do you look for to predict whether a system will be easy to adapt? It's hard to actually know the answer unless you've worked with different systems, and you've seen how the exact same problem can be much harder on one system than the other. Unfortunately, the ability to evaluate design is much more about tacit knowledge, rather than explicit principles. Still, I want to say there's rule of thumb that anyone can apply, and that is \"easy problems should be easy.\" You can usually get a good sense of whether a system is productive by trying to do the simplest possible addition (and remember additions are easier than changes). Was it hard? Then the system probably has a design problem. That's the difference between good code bases and code bases that are a struggle to work with. I've noticed that the good code bases will often have a tutorial that proudly demos just how easy it is to get started, because the designers care about it and want to make it as easy as possible for others to collaborate. Of course, a well-designed system has its own trade-offs as well. You probably don't need a good design unless multiple people are working on the project, since it's enough to \"keep it all in your head.\" You might also see systems that are \"over-designed,\" in the sense that they're trying to solve problems that might exist in the future, but don't exist today. But it's worth revisiting the question about design on every change that gets made to the system you're building. Does this change make it harder to solve the easy problems? If the answer is yes, then the change had better be doing something remarkable.","title":"What problems should be hard?"}]}